# 吹爆！这可能是B站最完整的（Python＋机器学习＋量化交易）实战教程了，花3小时就能从入门到精通，看完不信你还学不到东西！ - P24：第24节-Introduction to Clustering - 凡人修AI - BV1Yx4y1E7LG

大家好，欢迎收听经济学习与Python金融应用的第11节课，那么这节课呢我们继续上一节课的内容，讨论经济学习的理论方法，那么这节课过后呢，呃下一堂课呢是机器学习的第四节课，那么主要是机器。

我们利用我们之前学的这几章机器学习的知识，把它应用在量化交易上，并与qs trader对接，那么我们在下一堂课的一开始，会介绍什么是qs trader，包括qs trader的一些应用的例子。

那我们介绍完之后呢，呃我们会把之前提到过的机器学习，一些比较重要的model用到我们的量化交易中，那么呢并被并对它的return风险以及sharp ratio，还有一整个捉到。

包括equity curve等等做分析，那么我们这节课呢是机器学习，最后一堂理论课程了。

![](img/fdecbca367067ad28fc7706513273640_1.png)

那我们来先看一下我们这节课的大纲，那么首先呢我们会介绍clustering，也就是集群理论，那么集权理论与我们前面提到的，不管是分类还是回归有一个很大的区别，是之前的分类也好，回归也好。

都是有监督性学习，那么这里的clustering呢，是无监督学习中最重要的一个啊理论分支，那么呢我们介绍完集群理论之后，我们再介绍他在金融领域的一个应用，也就是对对一个SNP或者是对这个EREX等等。

equity或者是ETF，我们利用自身的fundamental的这些维度的信息，那么我们把满足某种类型的或者是一条equity，它不同时间段的类似的点给equate，在起aggregate起来。

那么这有什么好处呢，比如说新来了一天的data，比如说我们把它aggregate到第一类中，那第一类我们之前通过model training发现，第一类呢是一个up类，也就是上涨类。

那么如果明天的step被我们归到up类的话，那我们就有理由在今天我们enter the market，那在明天我们有理由认为呢这个stock会看涨啊，这是利用分类做呃，Algo trading。

那么第二块呢我会简单介绍一下神经网络，因为神经网络呢，跟我们之前的机器学习有所不同，它它因为比较复杂的响应跟比较多层次的layer，那么神经网络呢现在是这个deep learning。

非常火热的一块儿讨论方向，那么它会被归为deep learning，然后也在NLP，就是自然语言学习中有很大的应用，所以我们在这里不会深入的介绍神经网络，那么如果大家感兴趣的话。

大家可以自己去看一些关于深度学习，或者是自然语言处理的书籍，那么呢大家会看到他的entry嗯，就是entry level的这个basic的知识呢就是神经网络，那我们在这里我们介绍两种。

最常见的神经网络是比较好学习的，一种叫人工神经网络，或者我们叫前馈性，也就是最简单的一种，最符合我们我们想法的神经网络，那么第二条呢是recurrent neural network，也是递归神经网络。

那么我们还会稍微提到一个，卷积神经网络的应用，那么关于卷积神经网络的理论基础呢，我们在这里不会深入的介绍，那么第三块呢是关于非监督学习的，另外一个很重要的技术，那么在之前呢一直没有机会啊，解释嗯。

就是这个dimension reduction，然后它dimension reduction主要是两块，一块是feature selection。

那么feature selection在之前已经有过介绍了，比如说包括用了so用rage regression做feature selection。

或者是在decision tree的时候用maximum entropy，也就是最大熵做这个feature selection，那么还有一块的dimension reduction呢是这个PCA。

那么p ca呢是无监督的，dimensional reduction跟这个feature selection呢，两个刚刚好是啊相仿的，然后那我们介绍完PC之后呢，我们再介绍他在金融领域方面的应用。

那这就是我们这节课大概会介绍的一些内容，那么对于对集群我们会有两套的代码，然后对PCA的话呢，我们没有在这里普建代码，但是呢大家其实就调PCA的这个包，其实是很简单的，我们后面会提到这个PCA。

关于如何改善这个clustering呢有很大的帮助。

![](img/fdecbca367067ad28fc7706513273640_3.png)

那我们先来看一下什么叫做无监督性学习，我们之前提到的所有的模型都是监督性学习，也就是我们有了feature x，那同时呢我们有我们的响应，YY不管是带标签的，或者就是category类的。

比如说up flat and down，这个就叫标签，那么呢如果是比如说price，那今天是100，明天是100。5，这个呢是叫做continues的numbers，那两种呢都是称为响应。

那么我们有响应，有我们的特征，那么这样的学习呢，我们称为监督性学习，也就是你学习出来的结果呢，我们会被已有的Y来进行监督，那么对于clustering或者是无监督学习的话，它的样本中并没有给定Y。

我只有我的feature x，那比如说呢我想要做一个星云的一个分析，在这个地球物理上，那我假设这个宇宙中的星星，可以在这个三维空间中一个点击，可以聚类，那我可以发现这些星团的那个之星。

那这个时候呢我的聚类就要找到这个每一个X，它潜在的label就属于星团一号，或者属于星团2号这样潜在的类别Y，那么之前是不会告诉你的，那我就把呢有同类别的，Y的这样的样本放在一起。

所以呢聚类后你就会获得一个又一个的星团，那么星团里面的点呢相互之间的距离比较近，然后不同星团之间的点呢距离呢就比较远，那么这个是无监督学习的一个例子，包括无监督学习的一个想法。

那么在这个clustering，也就是无监督学习的一个很重要的model里，我们的训练样本呢只有x feature，那没有Y，这是大家要注意的一点，还有就是这个无监督学习呃，跟监督学习相比呃。

他的想法呢更为简单了，因为这个时候没有regression，没有class clustering，大家就更freestyle，但是他有一个呃，目前为止都是一个很难攻克的问题，就是因为没有Y。

那这个时候就不存在training跟testing了，因为呃之前呢我们想做training，我们比较model之间的好坏，我们比较最小均方物，因为我们有Y。

然后有我们predict model predict出来的y hat，那我们对减，不管是取绝对值的和还是取啊平方的均方物，那我们都会有一个东西来量化它啊，当然是在无监督学习中，就是因为没有了Y。

那么我们并不知道我们聚类好还是坏，我们也不知道我们的k means出来的结果，到底跟实际的符不符合，那所有监督学习里面的feature，就是选选model的一些方法，比如说AIC，比如说BIC。

还有这个CV就是cross validation，我们之前一直提到过的，把样本分成训练集，分成这个test集，那我们用cross validation来交互印证，来算出我们这个model最近似的。

这个叫做test均方物到底有多少，那么对于无监督学习来说，就没有不存在任何模型之间的比较，那么我们只能根据啊，比如说pro出来的图，这个星团，我们因为我们有我们这个地球物理的知识。

我知道我这个星团分出来嗯，make不make sense，只能通过这个人为的intuition进行模型的判别，就没有任何这个量化的比较，所以这个是无监督学习比较可怕的一点。



![](img/fdecbca367067ad28fc7706513273640_5.png)

那我们介绍无监督学习，最重要的一个理论就是聚类，那么聚类里面的话K之K均值聚类是最basic的，也是应用最广泛的一点，那么我们看一个例子，什么叫做K均值聚类呢，其实想法非常的简单。

也就是我们最后聚类出来的每一个的质心，其实呢是属于它这个类别的这些点，它们的这个几何中心，所以叫做K均值，那K的意思呢就是我有K个centuries，就是K个之星，那么我们的具体的算法呢是如下所示。

那我们首先呢现在我们这个空间中，随机的选取K个聚类点，或者我们叫做置心点，那么英文上我们叫cluster centrates，Central，就是志新的意思，那就取K个先随机的取。

那么到后面呢我们就重复如下这个step，直到我们收敛啊，怎么做呢，就是我们对每一个点，或者我们将每一个样本I，那我们计算它应该属于哪一个，我们上面得到的这个质心，那也就是离它这个几何距离最近的。

这个字形点呢，我就把它归为这个类型，比如说第I个点我算出来之后呢，我离第二个字星最近，那么它就会属于第二类，那我们把每一个点都loop过之后呢，我们对于每一个画好的类别，我们重新计算它的质心。

那得到执行之后呢，就得到了第二步的这个呃，缪一到缪OK那么再进行迭代，迭代到什么时候为止呢，迭代到我们的这些点，就我们得到这些执行点，不再因为我的第二部loop而改变的时候。

就说明我的我的结果已经收敛了，那呃K均值聚类就已经做完了，那我们得到了这些mu，一到最终的M1到MK就是我们的知心，然后最后划分的得到的划分的类别呢，就是我每一个样本点应该属于哪一个类。



![](img/fdecbca367067ad28fc7706513273640_7.png)

那这个K这是K均值聚类的一个，就是关于这个新团分类的呃一个例子，那嗯在我们再重复一遍，就是说K均值聚类中的这个K的意思呢，是我们给定的聚类数，就我们有多少个类别，那么每一个点就是由CI来表示。

那么呢CI呃到底应该属于哪一个字星呢，也就是它到每一个字星要算出它的几何距离，算出几何距离之后呢，选取最小的，也就是我这个这一步current state的时候，我这个I应该属于哪一个点。

那算好了之后呢，把它们分类都分好了之后，对于每一个这个云团，每一个聚类团，我们重新计算它们的执行，我们update我们的执行，update执行之后呢，再对我们的每一个样本点再进行分类。

也是利用上面的这个几何距离最小，那直到我们这些新团的执行都不再移动了啊，那这就是我们完成了我们的这个K均值聚类。



![](img/fdecbca367067ad28fc7706513273640_9.png)

那这边我们可以看一下，最简单的就是比如说二均值只有两个，执行的时候，我们的step是怎么收敛的，首先呢一开始我们是blind的，我们什么都不知道他们属于哪一个类别，所以都画是绿色的点。

那这个时候呢我先随机在空间中选取两个点，作为我的呃，一开始的初始之星，比如说是这个红色的点，跟这个蓝色的点，可能一开始这个看起来很incredible，但是没有关系，他马上就会改变他们的位置。

然后呢我们确定了这两个质心点或者叫central之后，我们对于空间中这些绿色的样本点，我们进行举例比较，比如说这个红色的点，我们计算它到这个初始之星，和这个初始之间的距离，那我们发现它到这个执行更近。

所以它就归为红色的类，那相反之呢，这些离这个蓝色的差比较近的这些圆点呢，就归为蓝色的类，那我们归类好之后呢，我们对这些红，把这些所有的红点质计算它们的几何的重心，那算好之后呢。

那这个差呢就应该移到这个位置，因为是这些红点的质心，那么同理对于这些蓝点也计算它们的质心，那就移到这个位置，然后我们得到了这两个知心之后呢，又对这些啊原点再进行一次的，再进行一次的K均值，那这个时候呢。

就是算这些所有的点到这个红点的距离，那谁离红点近，那就是属于红的，那离蓝点近，那就属于蓝的，那我们在update这个执行，直到这个执行不再变化，大家可以发现是从第二步开始之后。

这个字心的这个变化就已经非常的小了，所以K均值的这个收敛速度呢是非常快的。

![](img/fdecbca367067ad28fc7706513273640_11.png)

那我们我我们来看一下这个K均值的收敛理论，因为可能看了刚才的这个几何直观之后，可能大家会认为就是说诶，这个K均值难道一定在这个上呃，这个空间就超平面和超空间中一定是收敛的嘛，其实是呃答案是肯定的。

但是呢有一个问题就是，K均值收敛不一定是收敛到全局最优，也就是我们K均值。

![](img/fdecbca367067ad28fc7706513273640_13.png)

其实我们idea里是想要得到我们的这个呃划分类，那么划分类满足什么条件呢，满足我的这两个类啊，我们这两个均值类的重心，所有的点到它们之间的这个距离的，几何平均的和是最小的，也就是扰动是最小的。

它能够非常完美的把这些点分成对应的云团，那这个时候我们想要找的其实真正的是一个global，也就是一个全局最优，但是K就所有的聚类其实都没有办法做到呃，一个全局最优解。

那这个时候我们就比较谁的local min，你们也就是局部最优解的。

![](img/fdecbca367067ad28fc7706513273640_15.png)

离我这个全局最优解会比较近，那么这个收敛理论呢，是因为我们的这个J也就是我们这个惩罚函数，或者我们定义为几变函数，那么这几变函数呢是非凸的，所以我们不能保证它的最小值是全局最小值。

那那k means的问题呢，就是在于我什么时候能够shrink到呃，这个全局最优呢。

![](img/fdecbca367067ad28fc7706513273640_17.png)

其实是跟我这个初始点的选取非常的有关系，如果我初始点的选取本来就离全区最优很近，那么OK我shrink到的这个local minimum，就刚刚好是global minimum，但如果选择比较远的。

那就很有可能他就陷入了某一个局部最优。

![](img/fdecbca367067ad28fc7706513273640_19.png)

它就没有办法再移动到别的点去了，那呃所以呢，这个k means对于一开始的初始点的选取，是非常非常的敏感的，所以对于run k means来说，我们的解决方法就是呢，我们会选取不同的初始值。

就initial centuries跑很多遍的k means，那我我们取其中最小的这个J，也就是惩罚函数或者叫畸变函数，对应的这个mu跟这个C的output。

那么作为我的这个最后我的king means的结果，那么这个是对这个k means，就只能收敛到局部最优的，我们的一个解决的改善方法。



![](img/fdecbca367067ad28fc7706513273640_21.png)

那么接下来让我们来看下一个Python的一个例子，让大家对这个k means呢有一个更好的idea，我们举一个例子是三个二维的一个高斯分布，那么在二维平面上的它的质心也就是均值呢，是这如下这三个点。

那我们再定义它们各自的弦方差，正也就是我这三个高斯分布的这个离散，离散程度，那么绿色的这个最散射，那么反之这个蓝色的是最centralized的。



![](img/fdecbca367067ad28fc7706513273640_23.png)

那我们通过Python来进行这个simulate。

![](img/fdecbca367067ad28fc7706513273640_25.png)

那首先呢我们要立，我们要import的是这个sk learn的cluster，里面的K均值，那我们在这边定义我们每一个高斯分布，每100个sample，那定义他们三个的这个中心和三个分别对应的。



![](img/fdecbca367067ad28fc7706513273640_27.png)

Coverance metric，就是协方差阵，那我们进行generate，我们的这个嗯二维的高斯分布。



![](img/fdecbca367067ad28fc7706513273640_29.png)

被我们并把这些高斯分布呢。

![](img/fdecbca367067ad28fc7706513273640_31.png)

就所有的点就打散都糅合在一起，融合在一起之后呢，因为我们提过这个chemist clustering，它不但对我们的初始的cluster选取非常的敏感，其实他对我设多少个K其实也很敏感。

我一开始并不知道这些点，到底应该聚成几类对吧，所以我可能能尝试三，那OK全是三，那刚刚好，congratulation在这里，你运气很好，因为我刚刚好其实就是三个这个高斯分布。

我进行进行simulate，那么取三就刚刚好fade我的model了，但是在真实的这个real world中，你根本不知道，比如说我这些股票，它们到底应该分多少类，所以你就会尝试不同的类别。

那我们在这里先尝试一下三类跟四类，看一下分类出来的效果如何。

![](img/fdecbca367067ad28fc7706513273640_33.png)

那我们在这边有这个arguments，就是n clusters。

![](img/fdecbca367067ad28fc7706513273640_35.png)

那么我们第一个model命命为三，第二个model呢命为四。

![](img/fdecbca367067ad28fc7706513273640_37.png)

那我们把我们这个三跟四个呃，means clustering呢，PSH来。

![](img/fdecbca367067ad28fc7706513273640_39.png)

那我们可以看到，如果我们也这个K等于三分类呢，我们画出来的结果。

![](img/fdecbca367067ad28fc7706513273640_41.png)

就跟我们的这个高轩distribution的这个图非常类似，那如果是四类的话，大家会看到，对于额这两个比较centralized的这个高斯分布的话，它还是几乎是正确分类了，只有几个点。

它可能miss classify了，但是呢对于第一个比较散色的呢，它就啊比较愚蠢的分为了两类，那就说明其实这个时候三类game makes sense，其实在四类中，我们假设我们其实是不知道。

我们是用三个高斯分布进行generate的，我们也可以看出三比四好，为什么，因为第四类其实大家可以看到，这个黄色跟这个绿色其实是其实是没有道理，把它劈成两半的，因为这三个类呢刚刚好它们之间的距离比较远。

所以我们通过直观认知，我们也可以认为就是分成三类会比较来的好，所以这是k means呃，就应该说是一整个无监督学习的一个主板，一个缺点，就是说我们的这个效果到底好不好。

最后是只能通过肉眼和我的intuition，比如说monket intuition，或者是这个地理的intuition，来决定我的这个分类到底是否是优秀的。



![](img/fdecbca367067ad28fc7706513273640_43.png)

那我们比较一下我们的这个真正的真实的model。

![](img/fdecbca367067ad28fc7706513273640_45.png)

应该有的情况我们可以发现三类确实很好。

![](img/fdecbca367067ad28fc7706513273640_47.png)

那么这个是我们simulate，那在日常中我们是不会有最后的一个这个涂的。

![](img/fdecbca367067ad28fc7706513273640_49.png)

那么我们就所有的点其实都是一个颜色的，那么你得通过我们对不同的K的比较。

![](img/fdecbca367067ad28fc7706513273640_51.png)

再加上我们的intuition，我们的知识。

![](img/fdecbca367067ad28fc7706513273640_53.png)

或者说我们的常识来判断这个K应该取多少。

![](img/fdecbca367067ad28fc7706513273640_55.png)

所以呢就是对于这个kimmiss class stery，我们需要最优化的是我们的这个K，那么最优化的方法呢是只能通过我们的market。



![](img/fdecbca367067ad28fc7706513273640_57.png)

或者是这个living的experience，那么接下来我们看一个在金融上的应用，也就是这个open high low and clothes，就叫做我们俗称是OHOC的一个classroom的分类。

那我们的想法是什么呢，其实是本来不是四个维度吧，我们把它们都除以这个open，那我们把它降为三个维度呃，那e。g。对SMP发PH指数，我们它我们把它构建出这三个维度，那我们根据这三个维度呢。

对它的这个performance进行分类，那这个分类的好处呢，就是说我比如说这个我们已经训练好了，我们分类成三类，那我们ideally觉得他的三类应该会聚类成，聚类成这个长。

然后呢跌和一个flat聚成这样子的三类，那如果我先获得了明天的这个high low and clothes，那我们比如说它分类到了第一类，那第一类是长，那我们就可以有理由在今天快收盘的时候。

我们我们进行log，那么明天呢我们就能够获利，那是这样的一个想法，那我们看一下我们具体的代码是怎么样实现的。



![](img/fdecbca367067ad28fc7706513273640_59.png)

那我们需要import非常多的东西，包括这个嗯three lid p，然后包括这个kindle stick，也是画我们的这个叫做竹状足状图，就是如果是黑的的话，是跌的红的啊，黑的话涨红跌这样子。

那这个的话跟中国的convention是刚刚好相反的，对吧，那因为美国人可能觉得红色代表一个warning。



![](img/fdecbca367067ad28fc7706513273640_61.png)

所以说红色的话是爹的意思，那我们还要import pandas，Data reader，import这个clustering。



![](img/fdecbca367067ad28fc7706513273640_63.png)

这个means k means clustering，那我们这个get open normalized price呢，我们主要的作用呢就是把我们破下来的data，转换成我们三个features。

分别是high to open ratio。

![](img/fdecbca367067ad28fc7706513273640_65.png)

low to open ratio跟close to open ratio，那我们clean好。

![](img/fdecbca367067ad28fc7706513273640_67.png)

我们这个data frame进行return，那这个plot candlestick呢，就是画出我们的这个蜡烛图。



![](img/fdecbca367067ad28fc7706513273640_69.png)

就是柱状图，就是大家看这个，比如说看盘的时候经常会看到的这个图片。

![](img/fdecbca367067ad28fc7706513273640_71.png)

那我们plus three d normalized candle呢。

![](img/fdecbca367067ad28fc7706513273640_73.png)

意思就是我们分类之后啊，我们把它画成一个三维的。

![](img/fdecbca367067ad28fc7706513273640_75.png)

那proclustering orders呢，就是当我们做好分类之后，再把他的candles的图给画出来。



![](img/fdecbca367067ad28fc7706513273640_77.png)

那这个cluster matrix呢是一个转移矩阵呃，我们接下来会介绍这个转移矩阵，大概是就是我们怎么解读它的。



![](img/fdecbca367067ad28fc7706513273640_79.png)

那么在这个main函数中，我们我们读我们读data。

![](img/fdecbca367067ad28fc7706513273640_81.png)

包括调我们上面我们已经clarify好的函数，那我们这边用的是这个cma groups，是用的这个sm sm p index，用的是框斗破，因为最近这个雅虎对data a limit管的比较多。

那宽LE其实也是，他每天只能对于对于这个匿名账户，也只能破额破50条，但是对我们这个就是学习叫research的话已经足够了，那如果对于真的daily呃这个trading的话。

那肯定还是后台得带bomberg terminal的那啊。

![](img/fdecbca367067ad28fc7706513273640_83.png)

我们这边这个第一个图形画出来，就是我们这个SNP的这个kindle图，那这个红色是跌嘛。

![](img/fdecbca367067ad28fc7706513273640_85.png)

然后黑色是这样上涨的，那这个是intro day，就是daily的这个kindle图，那这个图呢是我们做好了分类，我们是认我们assume分成五类，就是flat。

那么带一点点的这个上涨的ratio比较高的，就是close的时候比open来的高的，然后这个呢是反之啊，绿色点是反之，那上面的这个湖蓝色的点，跟最下这个黄色的点呢。

也就是volatility非常大的类别，可以大家可以看到上面的这个outlier是很高，那下面黄色的也是一样，就是这个low to open ratio非常非常的低，那上面的话是反之。

就是close to open the ratio呢很高，就说明这些湖蓝色的点其实是一个ball market的，那这个下面的这个黄色的点呢，就是就是一个bear markets的点。

那中间这些比较centralized的就是比较平缓的时刻。

![](img/fdecbca367067ad28fc7706513273640_87.png)

那么即介于在这个均值回复中间这一段的时刻。

![](img/fdecbca367067ad28fc7706513273640_89.png)

那我们可以来看一下，我们最后prod出来的这个candles的这个图。

![](img/fdecbca367067ad28fc7706513273640_91.png)

大家可以看到分为的这个五类中呢，其实最宽的点就是前面两类，也就是不是很确定的一个情况，像第一类是最宽的，就是红的跟这个黑的呢是这种非常散的，离散的分布的，很难describe它到底是这个是涨啊还是跌啊。



![](img/fdecbca367067ad28fc7706513273640_93.png)

因为既有红色有黑，其实也就是对应我们图中最centralized这块紫色。

![](img/fdecbca367067ad28fc7706513273640_95.png)

这个区域我们称为是一个这个waiting time，也就是说我们很难take action到底是long还是short，那第二块呢这一块大家看到也是红黑间隔。



![](img/fdecbca367067ad28fc7706513273640_97.png)

但是黑的会稍微多一些，那就对于我们图中的上面的这一块儿，就是这一块这个酱蓝色的，那么这一块呢也是属于，就是说它是一个小波动的。



![](img/fdecbca367067ad28fc7706513273640_99.png)

但是依旧是一个上涨的一个区间，那么这一块呢也是一样的，这一块呢大家会看到这个上涨，上涨的这个幅度就更高了对吧，因为黑色的竖线就越越粗，并且呢就是红点要比上面来的越少。



![](img/fdecbca367067ad28fc7706513273640_101.png)

而且这些点更为centralized，那就是其实是对应上面最上面的这些点啊。

![](img/fdecbca367067ad28fc7706513273640_103.png)

然后大家可以看到剩下的这两个红点，剩下的这两个红的，其实跟上面这个也是很类似的，就是这个的上涨跟这个的下跌非常的像，那么这个上涨呢跟这个下跌很像，就是竖线呢非常的长，但是呢他们的这个时间区域就越集中。

其实这个跟volatility clustering，其实也可以联系起来，什么叫呃ventity clustering呢，就是在一段时间内，它如果是下跌的，而且下跌的非常狠的话。

那么他更有理由响应到第二天它也是下跌，而且也下跌的很狠，这就叫做一个呃，相关性非常高的一段时间的效应，就这一段时间它不但下跌了，而且它的波动率也非常高，就是高波动下跌，那么这一段就是一个非常危险的时间。

那么对于上涨也是一样的，它这个时候上涨呢，它上涨幅度又大，而且它的波动率就越高，也就是在上下这两个区间呢，其实风险很高的，也就是对应这一块最最窄的这一块，上涨区间跟这一块最窄的这个下跌区间。

那比较安全的上涨和下跌区间呢，就是这这一块跟这一块就是第二块呢跟第四块。

![](img/fdecbca367067ad28fc7706513273640_105.png)

所以这个是我们clustering，对于我们一整个这个market分类的一个应用。

![](img/fdecbca367067ad28fc7706513273640_107.png)

那我们接下来呢我们介绍这个neural network的。

![](img/fdecbca367067ad28fc7706513273640_109.png)