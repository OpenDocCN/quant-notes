#  - P6：第06课_机器学习与量化交易项目班 - 大数据狂人 - BV1V24y1Y7eY

![](img/5fffcd7617edf55f8c65cda7ac10ad9f_0.png)

好那我们现在可以开始今天的这个课程，好吧嗯，今天讲的东西比较多啊，把差不多四次课的时间我争取压到一次，因为大家点菜点的太厉害了啊，咱们来看啊，今天大概讲这么几件事情哈，主要第一个是啊。

今天的主题是特征选择，就是基于我们上一次在这个课程啊，就是已经，如果当你已经有一个这个原始的，所谓的肉data之后，我们怎么样的通过各种的这个啊啊啊，所谓的这个啊transform就相当于一些变化。

变成我们所需要的在每一个时间点上，每一个time，它所对应的这个x和相相应的这个y值，和你想预测的这个y值，这个叫做所谓的这个训练集对吧，我们上节课已经会嗯，嗯主要讲的就是说我们怎么制作这些训练集o。

那么这节课我们的关键是，我们怎么样把这些训练集进行，第一步的，就是说我们把这些训练题做一定性质的，所谓的这种你可以看成一种优化，而这个优化的方式呢叫做feature selection。

就是说因为我们现在上节课选的这些x，都是拍着脑袋选的，经常一选可以选300位，但是300v有可能有的是冗余的，也有的是否有可能是啊，每一列它的数值都一样啊，这个也是很有可能的。

比如说你不小心选了一点进去全是零啊，这怎么办啊，那么我们今天就学各种各样的优化方法，来选出来一组比较比较这个啊，嗯这个嗯开了挂的这个x，因为当你x的质量好的时候啊，你用一个这个啊分类器才能分出啊。

比较好的一个结果，当你x的质量很差的话啊，你用再好的这个分类器也很难得到嗯，一个很好的结果，ok那么大概是讲这么三件事情，一个是特征选择的一个主体的，这个呃呃呃呃一个一个方法。

就是说特征选择的这个过程是什么，和它的意义在哪，另外呢给大家啊介绍一下遗传算法怎么用啊，然后呢基于这个我是特别的想讲，就是嗯这个就是嗯真正的让让大家真正理解。

所谓的back propagation这个算法，因为其实现在讲各种深度学习的嗯，博客也好，书也好，还有各种各样的课程也好，他们更多的是关注的是这个网络是谁跟谁连的，但是呢如果你要做稍微体面一点的。

真正应用的话，你如果不能真正的掌握bp算法的本质的话，你就很难嗯理解一些目前比较先进的一些呃，关于神经网络的一些应用，然后呢再基于基于基于这个bb算法，我接着往下呢给大家介绍一下。

您怎么样用这个嗯r n为什么要要讲这两个呢，就是遗传算法跟这个神经网络是两大并行的啊，玄学啊，就是嗯没有什么理论很强的理论背景，但是呢在实践中又极其有效，而我们在这个课里头。

我其实为这个事情也想了很长时间，我也很稳定为为难，就是说如果用2~3个小时教给大家，什么是贝叶斯方法，那这个事情实在是嗯太难了，因为大家的这个背景也不是太不是太一样，那么又得交给又得照顾到。

如果没有好的数学背景的同学啊，也要照顾到，我们毕竟是量化教育呃，呃与机器学习实战，又必须得让大家真正掌握几个，真正能够在实际中运用到的人工智能的算法，然后时间还有限，所以我想来想去，我觉得如果大家。

诸位今天能把这个神经网络和遗传算法，两个真正掌握了，并且能应用到实践中的话，嗯也是非常好的一个事情，这个是这两个是啊很棒的两个方法，嗯不会给大家制造太多的这个所谓的啊，各种的数学门槛也好。

还是其他的门槛也好，就相当于啊降龙18掌教你掌亢龙有悔啊，你拿这个别人来了，你都用亢龙有悔打回去就行啊，嗯就是说嗯比如说你一把ak 47吧，啊他也许比其他的这个课啊，比其他的工具啊。

看上去要不是那么的装逼啊，但是呢在实际实战应用中还是相当不错的啊，这是第一个第一个事情，那么呃大概就是然后我争取用两个小时啊对啊，但是啊想讲这两个的另外另外一个原因是，目前市面上讲这两件事的博客和书。

都都太不好了，我我目前没有找到一个比较体面的，真正能把这两件事讲清楚的一些东西，所以我觉得那也是我挑选这两个方法的，一个比较重要的原因，ok那我们现在开始上车好，那么先说上次作业哈，就是昨昨天就是啊。

不是前天周六，周六的最后一张ppt，我给了你们一个百度网盘的链接和密码，那个里头是我帮你们做好了一个训练集，training x跟这个对应的这个y，你要对y做一个预测啊，把那个real嗯y那一列啊。

就有一列都是零，你不管，那么那个是我加上去的啊，你就你就直接预测这个real y就行，然后呢就这个x我已经帮你都都提取好了，数据比较大，那么最后一个批呃，最后一个作业呢，最后一个作业就是啊啊啊啊。

我就我就帮你把数据已经做好了，你要干的事情，就是用各种的机器学习的这个这个方法，用各种机器学习的方法来来来，在这个数据集上做一个回啊，回归任务，做一个regression的一个任务。

做完regression的任务，你不是有他真正真实的y值吗，你有你这个prediction的这个y值，然后呢，你就要汇报你的真实的y值跟prediction的y值，在我给你的这个训练集上。

因为这个数据是跟时间有关的，如果你把它呃，就是把这些数据进行重新的排序的话，你很可能你的性能很好，但是是假的啊，就说你用前70%的数据做训练，你在百分之后 30%的数据上啊，请给我汇报你的这个r方值。

然后呢你最好你先用一个这个benchmark，最简单的方法，你至少用一个kn，用一个这个线性回归，你在我给你的数据全跑一下，汇报一下线性回归，在测试集上导入方值，汇报一下k n n在测试集上的r方值。

然后加上今天今天我会教你怎么样的选出来，从这个x中啊，那么多列中选出来一些比较好的子集，然后你再基于这个特征选择之后的方法，再做一个啊新的一个模型，然后再汇报最好的表现啊，这个是一个比赛。

第一名的同学是有奖励的，嗯有一个人说卡其他人卡不卡不卡不卡好，有一个人说他我就不敢好好，那我们接着往下走好，那我们就是这样啊，先用呃比较简短的时间，我们说一下这个特征选择的三种方法。

就是说我们x比如说我们的这个x，每一个时间时间的x我没有x1 ，一直到x p大概有p个，这个p有可能比如说等于300，也可能等于1000，也可能等于20啊，而这个这些x呢是你拍脑袋自己弄出来的啊。

就是说你啊你觉得这个东西也有用，那个东西也有用啊，什么东西你都掉进去了，这个是一个原始的x，那么基于这个原始的x来做，那肯定是不好的，对吧啊，那么怎么选呢，大概有这么三种方法。

第一种方法就是所谓的叫呃呃这个自己选择，就是说我们呢我们选出来啊，用某种方式你选出来，这不是300个的吗，我们选出来30个啊，这30个我们在这30个上做预测啊，这是第一种方法，就说我们选一个子集出来啊。

第二种方法呢是我们也不选一个子集出来了，我们让这些系数呢啊比较有规律的啊，比如说这个x啊，那么这个系数它就会自动的为零啊，这样的话就作为一种选择啊，那么这种东西其实呢我们之前的这个朗诵干的，就这事。

对不对啊，这个讲过了，所以说chk讲过了，然后呢降维是另外一种完全不一样的方法，它是什么意思呢，它是说这个子集选择我选出来的东西，这些x的值是不变的，我只不过是选出来的其中的有限几列。

比如说我一个300维的一个向量，x300 乘以一的一个向量，我用一种函数给他，就是说我用我用这个函数就能给它变成一个，比如说啊啊啊啊z它是在20维度上的，而这20维的向量是这300雷的某某一种。

线性或者非线性的一种组合，就是说这个这种降维的手法转换出来的东西，它的每一列的意义，跟原来的300列是一点都不一样的，就那就就说完全相当于变身了一下，变变身了之后啊，然后再做再做这个新的。

在这个新的数据集上再做啊，回归或者分类任务，比如昨天有人提提到了，说啊我用一个奥特曼coder行不行，没有问题，auto my coder就是降维的一种方式，pc也可以啊，s v d也可以。

还有各种各样的奇人技巧都可以，pc没问题啊，可以做啊，一般多少维度需要降维，这个不好说啊，呃降维的意思并不是说我想画到平面上，而只是想把一些某一些类间的一些这个，这个vance进行某某种的啊啊啊伸缩啊。

嗯那么我们今天先用啊，先看看最简单的我们怎么选一个子集，出了好吧，就是说我们选子集，其实它它难是难在哪儿呢，是这样啊，先说一下评评价标准，比如说我们现在有一堆这些x，有些这些x它好还是不好的。

那么我们就可以计算这些x跟这个所谓呃，对应的这个y值的方值，r方值是在r峰值，这个r方值大家回去查就会有一个公式，待会我会给出这个相当于一种指标，来判断回归任务的好还是不好，那么他最不好是零。

最好是一啊啊，那么就是说那么我们要选的时候，我们如果有pig向啊，pig变量，我们每个变量可以选择或者不选，所以说如果我们暴力的猜，我们最优的这个子集组合的话，那么它是指数级别复杂度的。

如果你要暴力的全部把它枚举一遍的话，你是枚举不完的，所以说就有一种呃，你要你没法得到一个最优解的话，那我们只能逼近这个最优解，毕竟这个最优解，最优解的意思就是说我们选出来这个子集，它的峰值最大啊。

这个就是这就是呃我们的目的，那么呃一个最简单的方法就是所谓的贪心啊，就是从算法上的角度就要开心啊，先看暴力啊，这个这个是纯暴力啊，纯暴力，就是说我们比如说，因为我们现在甚至不知道最有子集的。

比如说我们原来是300个变量，我们甚至不知道最优子集应该是五个变量，最好还是十个变量最好，更不用说如果是五个变量，最好是哪五个变量最好，那么我们能干的事情就是草，那就暴力吧啊我们假设最后只级是一的时候。

我们把那300个全试一遍，最有子集是二的时候，我们选300个，选两个啊，全部试一遍，最后只一三的时候，我300选三啊，就是排列组合嘛，我把所有可能性全部暴力的读一遍，录完之后呢。

我们汇报那个具有最大的r方值了，或者呢是这个最大的是a i c或者b c值的，那么这里这几个这几个这个评价指标，都是对这个特征的一种评价啊，具体哪个好，其实有的人喜欢这个b c，有的人喜欢a sa。

有的人用r风就蛮不错，这个是一个完全的个，没有什么很有道理的一个方法，它只不过是一个评价指标而已，那么这个是最暴力的方法啊，在这个最暴力的方法，当你的这个维度，比如说我只有五维的时候。

你可以暴力的玩一遍啊啊但是维度越高的话，你这样做就肯定就不好了啊，这样做肯定就是有问题了，那么一个所谓的这个想逼近最优子集的一个，你只需要走一个for loop的，就是o n o n复杂度的一个算法。

他是这样做的，他这样做的，他就说嗯，ok那么我们现在就是呃呃假设我们的x有x1 ，一直到xp，然后最优的这个是五，我们的整个的这个空间，然后最优的这个集合，我们现在刚开始是不知道的，那么我们就暴力呢。

我们先选一个，就是如果我们最有止境只有一个的话，他应该是谁，就是说最牛的那个x唉，就是最最重要的那个变量是哪个，那我们的暴力呢就单个的算一遍，比如选出来的x8 啊。

如果比如说这个这个300维的一个回归问题，我们只能做一个一维的回归的话，我们发现x8 是最好的，那我们就把x8 放进去，然后呢如果是呃再往上加的话，就我把x8 固定了。

不要像之前那样是n选二或者三或者四，我们就把这个x8 固定了，再在剩下的这些里头选，如果我要加一个，加哪个最好，就相当于每一步最好，然后就这么跑一遍，跑完之后呢返回最最好的那个组合啊就行。

比如说我加到某一步，我再加就加不上去了，那你就你就返回一个啊，只有十个变量的一个selection啊，这个是非常简单的呃，有一个比较有效的一个东西叫做forward selection啊，这个都很简单。

大家有问题的，还有一种方法是它是所谓的叫做backward，这个我就就不讲了啊，backwards selection，它是倒着来，他就是说我先把这这这300个我全都要。

然后我只比如说你只要我开除一个员工啊，我距离开除谁，那么我就一个一个的啊，我开出x一的时候，我算个r方值，开出x2 的时候，我算个r方值，开出x300 之后，我算一个r方值，我把把r方值啊。

降低最少的那个员工给他开掉啊，有你没你都一样，我帮你杠，就是倒着从最多降到最少，这个是嗯穷人的做法啊，现在先如果我这公司只招一个人，谁能给我这个公司带来最大的价值，先把那个人招上来，然后再招一个。

如果这个人已经入职了，我在这再该招谁，一直招到啊，不用招了为止，这个就这个就是很嗯，就是如果你啊早身份30年，这个算法，就是你的就是没有任何天才的成分在里，关于我说的前面这些所有的玩意儿啊。

这些东西都是日常有人天天在用的东西啊，嗯我说的很快，是因为我后面的东西更重要，我需要把时间留出来，大家有问题没有，x x一是一个维度，x8 是一个维度，整个x是一个向量，它是p为的。

计算的时候x是代入神经，我目前还没有神经网络呢，咱先不说神经网络的事哈，训练集和实际的训练集要想当然一样，因为我们算r方的时候，是在你训练集上算的，嗯咱们咱们就先问我讲的东西，降维的东西，我后面讲。

有不卡的没有没有没有，好好好好好，有一个不卡的，咱们说好啊，一个不卡咱就不说啊，好，那么现在咱们讲，真正我非常激情的和我想讲的两件事情，因为这两个东西我发现呃其实不难，但是网上讲明白的时代我没找着。

所以我觉得草那这个事情我还是讲一下吧，啊讲一下先讲啊，第一个巫术啊，遗传算法，第一咱们讲genetic algorithm，这个遗传算法很牛的，这个遗传算法不仅是在特征选择里头。

我们在今后的一些选股的里头，他因为他是什么呢，他它它是一类没有什么，就遗传算法和神经网络都是基于生物生物现象，就是inspired，他们都是一些inspire algorithm。

就是说他们是从生物学中获得一定灵感的，一些算法啊，他是不是真实的对生物的模拟呢，你说这个是呃神经网络是不是大脑，它它它肯定屁也不是，但是呢它是从呃，呃这个脑细胞中获得了一些灵感。

而发明的一类数学模型一样，这个遗传算法是它从dna中啊啊啊啊，获得了一些灵感，他能干的事情是各种的，你的优化理论，比如说我有一个问题，我想找出最优的那一个啊，或者最优的那一个，可以是最优的一些子集。

也可以是最优的一种投资组合，就是这个最优是由你定的，他都能干，而且干的呢其实相当不错，相当不错，在很多地方跟这个神经网络是并驾齐驱的，它是广泛的应用在各种的领域中，在经营中就呃也不用说了。

那么基于啊大家的人，目前对这个呃呃呃这个呃呃进展来说，我觉得是时候把这个东西教给大家啊，如果会了之后呢，啊就是环游世交给傻姑那三叉子啊，你就能把吴某李莫愁能能能查跑，就是一点也不丢人。

你用一串算法一点也不丢人，如果把它用好的话，那么课后呢我会给大家赠送一本非常好的书，这个书网上买不着啊的一个电子版啊，就是网上没有盗版的一个电子版，然后这个这个书里头对遗传算法的各种实现，讲的非常好。

还有一些很好的例子啊，这个是后话，咱们先听啊，先听一专算法究竟是个什么玩意儿，好吧啊，那我们现在开始好遗传算法是这样，他的理论依据很简单，第一个理论依据是关于种群的，就是说比如说我们现在有一个一个种群。

有2000个个体，然后这2000个个体呢，嗯如果适应环境的就活下来了，不适应环境的呢就死了，然后活下来的人呢，然后生下来的子代呢不行了，又死了活嗯，好的呢又活下来了，然后大概这么1000轮迭代之后啊。

那拨人就是最优秀的啊，就相当于啊就这么一个很简单的理论，第二个是关于这个，那么每个人他的他的好坏是取决于它的基因的，所谓的d n a，那么大概用两分钟时间，给大家上一堂高中生物课哈，就这样。

这个是你的细胞，好吧，这是你的细胞，这细胞里头呢你有一个细胞核对吧，细胞核呢，比如说你是有假如说嗯嗯最简单的情况，一个生物它必须有两条染色体，两条染色体染色体上就是就有dna啊。

那么dna就包含了你所有的这个信息啊，这个呢是这个是你妈妈给你的，这个是你爸爸给你的，ok那么我们平时的这个细胞分裂呢，平时的细胞分裂就是简单的，你目前有的两个dna进行了一次复制，然后呢一变二了。

那么你就是你的体细胞，你就会一人得一对啊，没有任何问题，他们是完全的一种复制啊，那么性细胞是这样，性性细胞是叫所谓的啊，对减数分裂，减弱分裂它是这样分的啊，就是他分完之后呢。

他这每每一个细胞呢就啊只带一个了啊，就是说我进行分裂的时候，我不复制啊，那么每一个就只带一个了，然后这个呢比如说是啊嗯一个男小男孩的啊，这么一个性细胞，然后一个小女孩的呢再把她的贡献，然后这两个呢在。

但是呢啊呃就是说他们在从体细胞啊，变到这个呃这个呃生殖细胞的时候，这些东西就发就会发生一些变异，就是这个位点零啊，从从从从比如说从a就变成t了，这个位点从t就变，就就相当于有一些数就会就会变叫做变异啊。

然后第二个呢他会就是他会cross over，就是说他们会这个片段呢，就跑到这个片段上去了，就这两这两个片段就互换了，所以你你这个你这个生殖细胞的这两个东西呢，跟原来就不大一样了。

就是有的地方是不一样的，就是有的地方的数值不一样的，有的地方这个片段呢，这一段本来是这一段身上的，他们两个给交换了一下啊，这个是遗传算法的一个关键的一个灵感，就遗传算法主要是从这儿得到的一些灵感。

然后那么你产生的这个东西跟另外一个啊，小朋友的东西结合之后，就变成了一个新的细胞，注意哈在在变异跟这个呃，呃就是说在基因变异跟基因的这个重组，就是说呃这块的dna跑到了这块去，这块的跟他交换。

这个部分是只是会发生在，你奶奶跟你爷爷的这个身上，而而你爸爸跟你妈妈在造你的时候，他们两个的dna是不会产生这个事情的，他们就简单的一个结合了，这个是相当于一个最简单的高中生物学。

那么它里头有很多更复杂的东西，那么这个这个数学模型它没有，它没有考虑到，好那么呃那么那么就这么简单的一个事儿，咱们现在看看咱们怎么样呢，对这个事情建模，并且来寻找一些函数的最优点。

选寻找出来一些最优的一些事情，那么我们现在开始它其实很简单，它就是这样做的，他这样做的，他是这样，这不是比如说呃dna，它的表示方法是比如说a t c g啊，有的时候是啊。

这个呃呃呃t有的时候呃呃对应的是u啊，这但是生物学家是用这个啊，这个这个这个啊四个东西表示这个我们不好，他是生物学家干这个事情是base for，就是说它它是一个四进制的，这个没意思。

我们计算机一般来说都是二进制，我们喜欢用零一啊，那么这个零一呢可以表达你的这个问题的，任何一个一个选择，比如说举个例子，我们这个x11 直到x300 ，我们最想要一组，最想要一个最优的个体。

这个最优的个体代表了我要不要这个变量，比如说10011001，这组叫做这个叫做基因型，基因型，这个叫做表现型，叫这个是genotype，这个是phenotype表现型，就是说我们刚开始在建模之前呢。

我们要先要定义一种定义一个映射，就说我们想要的东西是这300个维度里头，我们选出有限的一个维度，而我们用基因来表达这个问题呢，就是说我们想要一个零一的这个向量，这个向量呢呃如果是一就代表我要他。

如果是零就代表我不要他，那么我们刚开始的时候，我们是不知道这个到底是啊，100110还是1000还是幺幺，这个不知道的遗传算法，就会通过啊50轮的这个迭代或者眼镜啊，就会给你找到一个最牛的一个个体。

能够存活下来的，这个是我们的这个所谓的问题的set up，我把问题先啊先先摆出来，大家有问题没有没有问题好，那我们接着看哈，接着看，就是这样，那么比如说我们现在有，我们现在有一个种群。

那么我们就要从种群中选那个最好的人哈，比如说我们有很多种群，这个种群的每一个个体就是它的基因型，比如说这每一个杠杆，就是每一个零一的一个串串，这个就是我们的所谓的基因型，对不对，那么我们刚开始的时候。

就是说我们刚开始是有这么一个种群，我们先初始化，就是我们编程的时候，我们先初始化1000个随机的零一的串，那么这个是我们所谓的刚开始的这个population，程度不一样，是因为我待会儿要画其他东西。

有挂票，但是其实它们的长度都是这个300维的，一个零一串，那么遗传算法的第一个呃，呃idea就是我有一个所谓的从population，他们的一个叫一个一个变异啊，一个叫做mutation，它是变异。

变异是什么意思呢，编译就是我随机的取一些位点，随机的在这些x呃，在这些d呃，呃这些零一串上，我选择一些数，让他从零变到呃，就是呃就是嗯让他从零变到一，或者从一变到零，这个没问题吧。

就说我随机的让一些这个呃dna的位点给它嗯，进行变异啊，就是让它的基因型啊进行改变，因为有的时候，我们比如说要求一个函数的极值的时候，它可以不是零一，它也可以是1。2，5。3啊，3。7没有问题。

那么在这种情况下，比如说我们要求一个函数，求一个函数的极值的时候，它的基因型也可以整，是一个实数的一个连连续的一个向量，那么我们这里的变异，就相当于我们对随机的以某一定概率，那么这个概。

这个概率就是我们预先的一个超参数随机的，比如说以0。05的0。05%的概率，让每一个这个位点产生一个变异，那么它还还会有一个这个啊超参数叫step size，叫做不长，就是说我变异多少，比如说1。2。

我只能在正-0。1这个范围内编一个随机数，对吧，你不能你你你得限制他不能变异的太多，这是这个是遗传算法的，你能干的第一个事情，ok第二个事情是什么呢，遗传算法能干的第二个事情是这样。

我刚才也我刚才也说过了，第二个事情我们能干的呢是啊，所谓的这个cross over啊，叫做，他是什么意思，他就说我随机的选一些两两的一些对，让他在某一个区域里头互换，就说比如说这个这个种群中。

这这两个他没有变啊，这两个呢他以前是以前是嗯嗯，就是说他以前这一块，比如说我换一个颜色吧，比如在种群里头，这是两个，这是两个不一样的哈，那么我我们要进进行一个叫做crossover的，一个基因重组。

基因重组他干的事情就是他重组之后的结果，就是这条中这一段是他的，那么他的这一段呢是这个红色的这个dna的，就是说我们把这一段东西，我们就进行一个啊啊啊一一个swap，就是说啊他他的信息到了假的信息。

到了乙的身子上，乙的身信息到了假的身子上，那么这个选择也是有指数指数级别多的，就是你能够以就是他的这个随机性是非常大的，我能够非常随机的来做这件事情，对不对，这个是第二个第二个干的事情。

第三个干的事情呢，当我们有了变异了啊，有了这个cross over了，那我们呢就要从这个基因型到表表现型，我们来把这些人给他造出来，就是每一个基因型，我们对应的是我们这个问题嘛。

就是我的具体要选哪一个x，那么我这些表现呢我会定义一个叫做适合函数，fitness，就是大自然就开始选了，在这个里头大自然选的这个函数就是r方值啊，你给我一个呃呃组合策略，给你个r方值，然后呢。

比如说这个人他的峰值是0。8，这个人所对应的r方值是0。2，那这个人就死了，就是它对应的这个dna，如果是这个的话，那么就说明这个dna是不好的，这个组合是不好的，那么他的这个呃这个这个适合度。

大概比如说是0。7，那么呢我们在用一种阈值，我们把好的人选出来，选出来一个子集，比如说这是啊是啊四个dna啊，这五五个dna我们变成了三个，变成三个之后，把这些东西我们再迭代着再来一遍啊。

这就算是一个generation，一代人就这么产生了，然后就不断的这么来玩这个事情，当你玩十次的时候，你就会选出来非常棒的一些人，有问题没有，并不能保证收敛，就只能你就走到一定程度。

一般来说种群如果你足够大，就就是遗传算法牛的地方，就是当我们的种群足够大，因为我们的这个随机性非常的大，任何的非凸的问题就是突突问题就凸优化呃，遗传算法做是不在话下，极其快的就收敛了。

非洲优化做的甚至比很多其音技巧都来得好啊，之前0。2，0。8是你的这个函数的定义出来的，就是说我我我们的问题是，要找一个最优的一个东西，那最优这个东西你肯定要定义是什么，比如说我们要找一个函数的极值。

那这个函数你得知道，我们要找一些这个x子集的选择，那这个fitness就这个r方值，这个是你的问题的一个定义，表现型是所有基因的集合吗，不是表现型，就是说我们的这个基因所对应的，真实的问题是什么。

比如说我给你一个这个呃，在有有的情况下，表现型就等于基因型嗯，但是比如说在这个特征选择里头，我们的基因型是100101串，没有意义的编码，那么它背后的意义是我要不要选择某一个变量。

那么呃还有很多人要干的事情，是一个叫做求一个函数的最大值，比如说嗯举个例子哈，比如说我现在告诉你有一个函数，它是一个黑黑核函数，这个函数不知道它的系数是多少，但是你给我一个x，我会给你一个y。

我们现在的目的是什么呢，我们现在的目的是请你告诉我x等于几的时候，y能够最大，这没问题吧，这是一个呃很经典的一个问题，那么遗传算法怎么做呢，比如说这个x比如说30维的，比如说x是30维的。

那么呢遗传算法就是表现型跟基因型就一样了，我的x我随随机的生成一个30维的，一些一些向量，0。11。29。9，它是一个30v的，我比如说我随机生成1万个这么多的，这么多的东西。

那么这个里头的fitness是什么呢，这个里头fitness就是我们对这些这些个基因，我们每一个人我们算一个这个fx我谁大谁活啊，谁小谁死，因为我们要找最大的，这个还不是机器学习，还没有机器学习的事。

真实的y就在训练集里头啊，你的训练集必须得有个x得有个y监督学习吗，暂停两分钟的问题基于这个算法，遗传算法的理论很难，因为你们可以看到遗传算法和神经网络一样啊，它的地方就在于啊，没有什么理论。

但是就是works，所以学术界不喜欢啊，但是工业界很喜欢，因为他们不需要证明，他们只需要能用就行啊，怎么样，我马上就会跟你说啊，你看这个不就是选择遗传算法，这个用法就是在做这个就是在做这个啊。

这个这个变量选择中就可以用，我已经，其实我已经把做变量选择的方法已经介绍过了，这里没有y，这里的y值就是我们的要预测这个东西，因为我们有一个y值，我们有些x我们要计算一个r方，这个r方我们是会算的。

对不对，这里的y就是你，你你要做的回归任务的这个y值，这超酷这个这个东西，而且非常好的，就是它非常容易实现，唯一要注意的就是要最好能够并行化一点，因为我们动不动一生成就生成1万个呃，这个种群的数目。

这1万个里头，我们每一个都有可能进行一个变异，每两两个都有可能进行一定的杂交，那么你串行的写就就就不大好，揭示和一切优化问题，像我们选择一个特征的一个子集是一个优化啊，1a不以后我们要讲的。

比如说我们现在有1万只股票啊，再举一个例子，比如说我们现在有n只股票，这这这n只股票我们到底选谁，那么也可以把它看作一个选择的问题，就是001001啊，就是选择这支股票跟这支股票。

那么他的这个呃我们要不要做一些其他的东西，我们这些变量都能放进去，然后我们进行一定系列的遗传，我们选出来一个最最好的一个一个串，它背后的这个表现型就是我们要干的事情，他他他能做的东西太多了，都可以啊。

都可以有，我和我课后就会给大家一套，别人做好的一个一个东西，冯老师讲课不会说，光给你个算法，告诉你他没有办法实现的，股票这个例子就是我就是你要预测的这个值啊。

就是嗯price of t加一或者return of t加一，啊再说一遍，fitness的函数是你要定义你，你看你什么时候选，什么时候说，哎老子今天要用遗传算法，你得先有一个问题啊。

你的问题是要找一个函数的最优点呢，还是要找一组啊x的一个好的一个组合呢，那么你得先有这个fitness的函数，你就说我需要找一些东西，让他的这个fitness最大，但是我不知道怎么走啊。

因为我不会图优化或者非图优化，那么遗传算法能帮你找，所以说如果不知道fitness这个函数的话，你什么时候你就不可能会想到遗传算法，这两个是反的，就是你得先有fitness，就是先有你的问题。

你的问题现在是选择n个x，让这n个x的r方值最大，ok那这个r方值就是你的fitness，对不对，我要选出来一组这个x是这个fitness，就是说你给我一对x，我算一个fitness。

那么我算一个二峰值，谁大了，谁活谁小的谁死，对不对，那么如果这个同学你问非的例子是什么呢，在这个例子里头可以是r方值，在其他例子里，它可以是任何事情，就是说我们现在有一个问题。

就是说我们要有一个目目标啊，就是说一般来说是max或者minimize，一个东西，一个东西，一个fitness，他有一个输入对吧，finance of fi啊。

这个x我们现在是要max或者min一个x是变量，fitness是给定的，是你目前在量化中遇到的一切的问题，如果是一个最大值或者最小值的话，遗传算法就能够给你找到一些x，让这个x呢。

当你嗯在你给定这个非内里头算的时候呢，它是有可能取得最大的，他是这样嗯，这个不好说，复杂问题的迭代次数多，简单问题的迭代次数少，但是呢一般来说你迭代，比如说你已经迭代了，迭代了100轮了。

他就其实就已经有最好的了，你跌你多迭代了900多，不会造成，就是说你你你多迭代几轮不会有太大的问题，因为那个牛人的dna就抑制他的血脉，就就就就下去了，再说几个遗传算法中的一些细节问题，细节问题。

细节问题就是说我们我们再回头看这个图啊，这这个图很棒啊，这个图，大家这个图我还是不要再对他进行一些好，那么这个图那么现在有一个问题，就是我非的那次怎么选，ok我算出来这个分了，这没问题啊，0。80。

20。7啊，这个人的fitness是1。3，这个人的finish是50，因为我这个这个finish他它它不是一个概率，对不对，它它是我这个函数的一个实数啊，有的非说不定的吗，还是负的呢，怎么办啊。

然后那么我们怎么从这个总群里头，我们选择一些嗯牛人的这个这个血脉出来呢，嗯那么一种比较简单的这个选法呢，它就是这样，就是说这是第一种选择的方法，第一种选择的方法就是说我用概率来选，概率选是什么意思呢。

比如说我这个啊，我目前的这个population，这个群体中有有1号2号3号4号5号6号人，然后我就一个一个的看哈，比如说1号人的fitness，比如他是1。2，它是0。3，他是0。1，肯定死。

必死无疑啊，他大概是1。5啊，这是啊单点他的fitness gal，那么我们是我们每看一个人，我们就随机的扔一个random number，random number，比如说它是0~1的一个数。

那么嗯就是说我我以某一定的概率来选择，要不要这个1号的人留下来，那这个概率可以是什么呢，就是说我要不要选一这个人的概率，它可以是这个111这个1号人，他的fitness除以整个的这个fitness。

这个和那么他这样的话，就每一个人他其实就是一个概率了，对不对，然后呢那么比如说这个概率算下来，算下来之后，比如说是零点点0。6，那我那么就是说，我们想让他以60%的概率存活下来。

那么我们怎么样用程序实现这个事儿，我以60%的呃概率来要他呢，很简单的一个小trick，就是我们先让计算机随机生成一个数，看这个数生成的这个random number，跟0。6进行比较，如果它大于零了。

我就啊嗯要它，如果它小于零了，我就不要它，那么我们具体选的时候，我就这样用，每一个根据他的fitness来来生成一个随机数，看这个随机数是否大于一个阈值，因为这样的它的等效的命题就是。

我以百分之多少的可能性把它给要进去了，ok，他没有过拟合的问题，因为这个我们没过，拟合是函数的这个逼近的问题，而我们现在是找一个函数的最大点或者最小点，它唯一有可能是陷入局部最优啊。

他不会有过拟合的问题，不是小于0。6，是它跟0。6的差异，一球差，如果他呃，如果他呃，我看看啊，如果它小于零小于零，就说明我有0。6的啊，如果它小于零就要对好对，就是说我们用南派里头就是啊拿np。

run，dm。random这个函数，它就会在0~1之间有一个有呃均匀的产生，一个以均匀的概率产生一个随机数，如果它这个比如说0。6的话，就是0~1之间，如果他如果它落在了这个区域。

我们就要它这是我们的threshold 0。6，所以就它跟0。6比它的差小于等于零的时候，我们就把它要进来，小于等于零，大家说的很对啊，因为这些东西也是我啊，对啊，就是我们尽量的相信一个啊。

函数库所提供的伪随机数的质量，为什么要跟一个随机数比较，因为假如说我就是就是说，我现在要说的是要说的这个事儿，就是说我们如何实现以0。6的，就是以60%的概率，要一个人或者不要一个人。

那么有一个小的一个一个实现的一个trick，就是我们的implementation的一个问题，这是纯纯粹的一个implementation，怎么样实现这个事，就是我以60%的情况啊。

让这个x一放到我的这个列表里头去，你怎么实现的啊，你要你你你你实现的这个方法，就是我random出来一个0~1之间的实数，我让他跟00。6比，因为我run出来0~1的一个实数，它60%的可能性。

都会在这个0~0点六之间，因为它是均匀分布的，如果他不在0~0点六之间，就说明他是40%的那个情况，那么我就会比较，我这个砖弄出来这个数跟0。6比，如果他在0。6这个区间里头。

就说明啊那这是60%的情况，那我就要它了，这就是一个小的一个trick，ok非常好，那么就是说这个0。6是怎么算出来，0。6是基于fitness算算出来的，就是说这号人物。

我为什么让他以这么大的概率活呢，是因为草人家的fitness很大呀，嗯他的finish怎么又映射到概率了呢，啊很简单，我把他的fitness除以整个的finance和，就是你能你应该活的。

这个你应该存活的这个这个概率，啊另外一种基于概率的，基于概率的衍生，就是我基于rank，基于rank来的，意思就是说呢，比如说嗯我也不需要非要定义出来，那么硬的一个概率，我只要让我表现最好的那个人。

以一个比较大的概率活下来，如果没有选到他的话，我以这个概率选择表现，第二好点点点，这么往下走，然后呢，呃目前大家实现的一般是要有两个这个事情，一个呢一个是基于他的fitness。

另外一个呢是要这个种群的diversity，就是说，比如说我现在已经选了1号人，2号人跟5号人了，然后呢我要计算这个群体里的这个vari，然后我要求我每加进来一个人，我要让这个种群的多样性要尽可能的多。

就说我加进来的下1号人，最好要让这个种群里这个vari最大，谁最大，谁进来，在在他的这个fitness比较相近的这个前提下，这个有点像我们这个这个嗯学过微观经济，就知道这些都是有taifunction。

那么就是说我的diversity就是种群的多样度，跟我的fitness它是有这么一种啊，就是说啊，就他们的贡献是一样的，我们尽可能的在这些线上选择，选择它的一个组合，就是说我非得很高了。

但是如果我不能对这个种群的多样性，进行一个贡献的话，我有可能妈的选的十个人，这十个人都是一个模子里刻出来的，那么根据啊，这个达尔文的这个这个这个这个这个理论，如果一个总群只有一个人的话。

就是如果一个种群里头他没有多样性的话，很可能它就会到一个函数的一个局部最优，它就出不来了，所以尽可能的要让这下x长得都不大一样，这样的话才能够让大家在这个函数的寻址空间，中的各个地方保障啊。

避免它达到一个函数的局部最优的这个问题呃，多样性的度量标准可以由你定义，这有各种各样的千奇百怪的度量标准，你一个比较简单的方法，就是计算它的这个这个这个，这个这个这个这个这个这个virus。

也可以是比如说你计算就是他的，你可以计算它的entropy，计算它的熵值，这个这个问题应该这个你应该你应该想想，应该是能回答出来的，因为我们在决策树里头，我们决定要不要切这一刀。

其实就是说要让它的多样性越来越小，就是要让他们的这个越来越不混乱，所以我们用这个熵值也能定义它，没有问题，最终选出来的是一群人，而这群人在这个函数上的性能都要么是最低的，要么最高。

就是说都是你想要的那一群输入，要的是一群书，我们的函数是不知道的，我们也完全不想知道这个函数是什么，就是机器学习是想通过一些点来，你和这个函数是什么，而这个优化理论是我把函数固定了，你这个函数。

反正呢我能不断的给你个x，你给我返回个y，优化理论，是想找出一个函数的最小值或者最大值，不是，最终选出来的就是我的这个问题的这个输入，这个输入这个东西，那么在这个里头，在我今天举的这个例子里头。

这些10010它可以是一种feature，但是在其他例子里头，我完全可以是就是就是实打实的x，我并没有进行特征选择，比如说比如说我心里头想了一个函数，是f等于x平方，但是我不告诉你。

然后你就问你就说一我给你一的话，你给我几，我说你给我一的话，我给你一，然后你又说我给你十的时候，你给我挤，我说你给我十的时候，我给你100，然后你就会又问哎，我给你25的时候，你给我多少。

我就说你给我25的时候，我算一下评分2525，我就给你625，然后你现在的意思就是说，遗传算法能够就这么长出来很很多轮，它就能找出来一个x，它能够让这个函数最小为零啊，它就涨了一些之后，他就发现啊。

那我给你零的时候，你就最小了，然后就嗯不错，你猜猜对了，那么这些东西就是遗传算法的输出，所以它可以不是一个特征选择，它是一个优化问题，而在我们这个例子里头，这个fitness呢。

它是一个呃想做一个x的这个自己选择，非常强大的一个东西，因为它这个是好，那么今天给大家留留一个作业，关于遗传算法的一个作业，玩儿这个事情啊，或是这样，我们现在有这么一个函数，这么一个函数是什么呢。

你输入一个字符串，就是就是是这样，我们就是嗯，重重说一下啊，就是说暴力的不是暴力的密码猜测问题，用一方算法来猜猜密码好吧，猜你的这个password，比如说我要求大家输入一个字符串啊，hello，空格。

你也可以写100多一一，就是你能写100多个词，无所谓，就是hello world吧，比如说你现在我需要要求你写一个遗传算法，来猜出来这个字符串是hello world，我给你，我你能有什么呢。

你能有的是一个函数，这个函数是什么呢，输入一个string啊，输出的是什么呢，输出的是你，你对了的对了的字母的个数，number of characters，which，is correct。

比如说嗯我真实的值true value是hello，word h e l l l l卧槽我居然不会拼英文了，h e l l o对吧，比如说我输入一个f，比如说你输入一个h e什么什么什么什么什么。

其他就在那胡说呢，那么我输出就就是一个二对吧，如果比如我输入一个h，我输出是个一，然后我输入一个啊，今天的纽纽纽约时报那么长一串东东西，你输出就是零对吧，那么我们怎么样用遗传算法来做这个事情呢。

啊其实很简单，算法就是那么你就生成，那么你的d n a其实就是有26种可能性，对不对，那么你就能生成一个字一个字符串，然后就是啊嗯零到每一个位点，它就是0~26的这么一个应该是27。

因为你还得包括空格对吧，0~27的一个数，比如说他是一，他是25，他是啊六啊等等这些东西，然后你就生成生成很多组种群，然后通过不断的这个杂杂交跟这个变异，然后呢是的差不多啊，1000多次啊。

就会猜出来这个hello world的这个结果，代码很简单啊，我回头也会发给大家啊，然后以及包括怎么样用遗传算法来做这个，八皇后问题，还有用遗传算法来选股啊，这个都这个都会有。

好遗传算法有什么缺点和限制，ok遗传算法的缺点是，如果当你的你产生的这个种群数目比较小的时，候，他很难找到一个举全局，就这这个global就是说全局最优点，就说你的种群最好要大。

你尽量的要并行迭代的次数要比较长，它的缺点就是有的时候不是太好预测，来做这个事情，就说他跑到什么时候是个头呢，就是遗传算法的问题，跟深度学习的问题是一模一样的，嗯我们不知道是为为为什么，他就他就ok了。

它不适合的场景，不适合这个场景是，如果你的问题是一个凸函数的话，你直接就用cd就做，就就就可以，比如说纯粹的在寻找一个函数的极值这个问题，再加上如果这个函数它是连续的，你还知道它的一些嗯这个解析式。

比如我告诉你x y等于x平方的时候，你就直接用图形化的方法就能求出b式解了，你就不你就不用，这它是一个呃也不能说是黑河吧，因为它是它是一个非常所谓的演化计算啊，想在演化界上做理论是很难的一件事情。

现在目前有一些人在尝试，但是也没有做出一些大新闻，关于遗传算法选择特征子集，建议大家去看这个算法，pose去年这个内部的一篇论文啊，是一个，某一位老师做出来的啊，很不错的一个方法。

就是说这个东西就是就是利用了一些遗传算法，来做这个自己选择的问题，做的还可以，那么其实用我刚才说的那个方法就就能做，这个是为了一些嗯，很就算就这个算法是背后是有理论保证的，最后给大家留。

因为我待会还要讲后面很多的东西，给大家留三分钟的时间进行提问，我一会儿回来好，我回来了，那么我们接下来啊遗传算法就告一段落了啊，我们接下来讲神经网络啊，这个神经网络的真正的原理我永远都怎么讲，多少遍。

我都不累啊，那么我们给我，我给你们讲一把，你能听到的最好的一次关于bp啊，不是关于卷积神经网络，那个是搞图像的人用的好，下下面我们看这个a n n，因为上次讲的还是太糙了，如果大家真正要能用的话。

我那么几句话嗯会的本来就会不会的，听完还是有点晕啊，今天让你不晕，好好好，我们赶紧上车，因为今天有可能要拖堂，因为后面的东西实在是太有意思了，好当然他是这样，刚才我们基于种群。

我们想的这么一套很民科的方法，但是极其有效啊，我在课程快结束的时候给大家看一段视频，让大家看看遗传算法能做多么令人惊讶的事情，然后这个n一样，a n也是一帮民科啊，读了一些这个高中的生物学知识。

然后搞出来的这个目前的深度学习的时代啊，他这样先说一点点的生物学原理，你脑子其实啊我们现在之间的交流，就是我们我的这一坨脑细胞，在跟你的这一坨脑细胞，控制了一些肌肉进行发生，然后通过一些电子的这些设备。

传到了你的嗯耳朵里头，然后又汇报给脑细胞，所以我们现在在交流的本质，这么想其实还挺挺令人起鸡皮疙瘩的，就是其实现在就是一堆脑细胞，在跟另外及堆脑细胞在进行一些一些交流，那么脑细胞内部它是什么东西呢。

这是脑细胞，这是脑细胞的细胞核，每一个神经细胞呢，它它它有一个很长的这么一个叫做轴突的，一个东西，好它上面呢它有一些接收信号的一些东西，他们这个东西呢叫树突，每一次呢就是说这个轴突轴突的这些东西。

它就是如果别人给就这是另外一个神经细胞，它的轴突是跟他接着的，然后它也有它在，它也有一些这这些啊，啊数图，来接受别的这个神经元，对他的这个产生的电信号，那么一每一个神经元它干的事情，其实就有一件事情。

他决定是否要向下一个神经元产生电位刺激，比如说当他受到的这个呃环境给他的刺激，大于一个某一个电位，大于一个阈值的时候，他就决定发射啊，它就不它就会发射一个，比如说啊一个零点值是0。6。

这么大小的一个电信号，那么这个电信号它怎么决定它是不是要发射呢，是根据他的这些轴突啊，他的这些轴度，比如说这个人给了他0。3的电信号，这个人给了他0。1的电信号，这个细胞给了他0。9的电信号。

那么他们把这些电信号啊加个呃，全部加一个加加加权求和啊，那么加权求和如果它大于一个一个阈值，如果给他的刺激不够强烈，他就说啊，我就不吭声啊，就是这么简单的一件，其实脑细胞比这个要复杂的多啊。

但是生物学家呢，其实他也没弄明白具体的事具体是什么机制，但是目前的这些机制，生物学家说ok没错，你们说的是对的，但是呢其他机制呢，呃其实生物学家也不也也不是太清楚啊。

现在脑科学还是属于一塌糊涂的一个阶段，那么我们现在就想用数学模型，怎么把我刚才说的这些话给他搞出来，我们怎么把，就我们怎么样用数学，把我刚才说的这些话表达出来，好那么嗯嗯其实很简单啊，其实很简单。

就是这样，我们这不是啊，这不是有这个就是我们的输入，其实就是这些x啊，比如说我x是300维的一个向量，有一个300维的一个向量，我们输出对应一个y值对吧，这个y值比如说是一是一维的。

那我们的输入最简单的方法的，比如说我有x11 直到啊，比如说这个嗯x n对吧，这个就是我轴突啊，就是我啊树突就是我收到的这些信号值，收到的这些信号值呢，我们是有这么一个要求，一个求一个每一个信号值。

我们对应的一个权重，对不对，x一和w一我们算出来了一个数，然后这个x n呢，它也会有一个这个相当于给他放大的这个倍数，这个权重w n我也有一个痣，然后呢。

我们把这些值呢用一个简单的一个summation，给它加起来，这没有问题吧，我给它加起来之后呢，算出来一个数，然后我再用一个阈值函数，就是说如果它大于一个t我就发射为一，如果它小于t我就发射为零。

然后我就发射为一个零或者一，这就是刚才我说的那个神经啊，刚才我说的这个神经元的一个，最简单的一个数学模型，这其实是真正神经网络的老祖宗，perception感知器，就是这个事。

就是说我有一些输入的一些信号啊，我进行一些加权啊，乘以乘以w我我都给他加加起来，就是其实他就是说x一乘以w1 ，加上x2 乘以w2 ，一直到x n乘以w n给它加起来之后，我看看他是不是跟t比。

如果大于t我就输出为一，小于t我就输出为零，这是一个神经元啊，对这是一个神经元，有问题没有，刚刚的生物模型是有的，刚刚那生活模型就是我们假设他们的，他们这个东西，就是我这个这个神经的这个这个细胞。

我怎么样决定它是否大呢，它确实是生物学家会告诉你，它是有一个全职的啊，那么这是一这是对一个神经元的一个建模，那我们对一坨神经元的深度学习，或者说是多层神经网络，他们一坨深渊，其实这玩意就是你脑子啊。

这是你脑子，你脑子里有什么呢，你脑子里其实是有的是一堆权重啊，有一堆各种各样的一些权重，和一些各种各样的一些阈值啊，以一种很奇怪的方式连起来，然后输入呢是一些信号，比如说这个信号它是n维的。

它就是x11 直到x n，然后比如说这个x一到xn，它可以是一个图片，那么它就是一个嗯，比如说如果是28x28那个图片的话，那就是700 700多位的一个向量输出呢。

大概比如说它可以输出为两维的一个z1 ，或者怎样，那么我们现在其实要干的事情，就是我们现在不知道大脑这个函数是什么，我们要干的事情呢，就是说我们认为z这个向量啊，它是个两维的向量。

它应该是谁的一个函数呢，它肯定是输的一个函数，稍等啊，他肯定是x的一个函数，对不对，他肯定是，我的输出肯定是输入的一个函数，它还是谁的函数呢，它还是w的一些函数，对不对，这些权重啊，因为权重变了。

我这输出肯肯定就变了，我的输入变了，我的输出也肯定变了，还有谁会变呢，我的阈值变了，我的输出肯定也就变了，对不对，那么这个z值呢，它可以是它就是这个输入的一些函数和这个啊，w的一些函数跟t的一些函数。

而这个函数是什么，我还没告诉你，对不对啊，那么现在就是这么一个东西，那么首先要干的第一件事是我不想要这个阈值，这个阈值变得太多了，我玩不过来了，怎么办，我能不能不要这个阈值，有没有什么方法啊，考考大家。

啊其实很简单啊，有多简单呢，就是在我们刚才这个片子里头啊，刚才这个数学模型里头，我们再加一项，再加一项，让他的输入永远是-1，让他的输入永远是-1，然后我们乘一个再给他来一个全职w0 ，也放到这个。

也放到这个求和的submission里头去，w0 当w等于t的时候呢，其实就相当于把这个函数就往过挪了一下，挪完之后，它的阈值就就让大家的阈值都都为零了，大于零就就只需要跟零比了啊。

这个是呃神经网络这门学科做的第一个进步，就是我把阈值这个事儿干掉了啊，我相当于加一个这个叫做bioterm，就是我的输入有x一到x n，我再硬加出来一个常数项啊，这个常数项的这个权重。

你就可以解释为这个阈值，这样的有个好处是，我只需要用学习算法找出来这些w就行了，我不需要再找t了，因为这个t就是其中一个w的一个值，这是我们要干的第一步啊，挺巧妙吧，嗯对嗯，好，全职全职怎么定义定义。

定义就是就定义就是定义，其实你大概问的是全职怎么找，这个就是我今天要给大家讲的，用bp的算法来找，这个是真正人类智慧的结晶，是怎么怎么找这些w啊，就是说那么现在的问题就是啊，张三说w这个w等于0。3。

李四说他等于0。4啊，它应该等于0。5，到底谁好对吧，怎么样的找到一组现，所以现在我们的关键是怎么样的找到一组w，使得这个z呢美就在我们的训练集里头，比如说我们现在观测到了很多x输入，观测到了很多输出。

我们怎么样的学出来一组这些w，让这些w你所预测出来的这些z值，跟他越近越好，这是我们的目的，对不对，再说一遍哈，这句话就很重要了，我在这儿再写一下，就是我们的z值啊，它不是我们这个一个函数。

这个函数是你的大脑啊，或者说就是上帝才知道的一个东西，跟x这个是我们不知道的对吧，这个是股决定股票涨跌的一个东西，而我们不知道这个东西我们知道什么呢，我们知道训练集对不对，我们知道很多训练集。

我们知道很多的x跟很多的z，我们想b想想，拟合出来一个这个函数的一个一个近似，所以说呢我们现在知道的是，我们如果有一个g这个函数，我们输入一个x，我们输出了一个d，就是说我们现在有的是这个。

我们的这个一个神神经网络对吧，它里头有有一些w，那么我们想想要获得的，就是说让让我们定义的这个这个东西，跟我们实际观测到的这个f，就这两个东西越近越好，也就是说我们会有一个要要要定义一个。

我们的所谓的performance，它是什么呢，那它就是尽可能的要让这个z值，跟我们的这个d值越近越好对吧，z值是真实值，真实值，地址是我们的预测值啊对吧，那怎么样定义我们这个神经网络。

就是这一组神经网，比如说我们的参数固定了，它是0。2，比如说w2 是0。9，怎么样定义我这个东西很好坏呢，我给他一个最简单的方法是，比如说我给他求一个二范式就行，就是我定义定义是它跟它的一个一个差啊。

我再平方再乘1/2，为什么呢，你其实你是可以，如果你你p值你可以定位成z减d就行，但是呢z减d的这个函数呢，它是它是它是它在零处不好求导，那么我们一个一个想让他好求导的，一个小trick。

就给他变成一个二分式，一个平方式就行，那么我们现在的关键就变成了，我们怎么样的寻找一些w，使得这些w造出来的d跟真实的z值比，它这个performance啊啊就是啊越小越好啊。

就这个其实就是你这个cost，有问题没有，这其实也是任何监督学习都要干的事情，对吧好啊，这个任何监督学习都是要这么干的，对不对，那任何监督学习都要这么干呢，那我们现在就说任何的监督学习。

我们如果有一个p值，这个p值我们给它求求个负的吧，因为我这个人比较喜欢求一个函数的最大值，对吧，这个p值你给它翻过来，它其实就这样，p值，比如说给他来来个负号啊对吧，它可以是负的1/2的。

这个z减d的平方对吧，那么我们现在要求它的最大值，那么这个p是谁的函数呢，这个p它应该是w的函数，因为我们已经给定了x了，所以那么就是我你每给我一个w的组合，我就会得到一个p，对不对。

你没给我一个w的组合，我就会得到一个p，那么跟神经网络没关系，任何的监督学习，我要找一组最优的w，我们怎么做呢，啊就是所谓的这个随机梯度下降法，比如说我的w是两个的w一跟w2 对吧。

那么我每一个w一跟w的组合，我会对应一个我这个performance的一个高度啊，那么它的这个等高线呢，比如说他也许是这样子的，那么我们要干的事情，就是找一组最优的w一跟w的组合。

让他能够在这个山山的这个顶端，就能够让他到这个p的这个最大的这个地方，这个是我们想要的一组，这个是机器学习学要学出来的东西，就是如果我给了你的训练集的一些x跟y了，请你给我找到一个函数的最优点。

就是请你告诉我w1 w一到底等于什么，能够让我这个东西最小或者最大，对吧啊，那么如果我们啊那么怎么找呢，你要暴力的搜，你肯定搜不过来啊，这个这个东西你你不能说啊，我每一举所有w的可能的取值啊。

从0~1万，我在每一举w2 的所有可能取值从0~1万，然后两两的在在暴力的事，那你这就就就这就就不好了，那我们要干的事情是什么呢，啊那就是所谓的这个梯度下降，我们每一次比如说我初始值在这。

我初始值在这儿的话，我w我怎么更新呢，我要更新为w减去阿尔法贝的是什么呢，阿尔法贝的这个p值对w值的这个偏导，比如说我每次怎么更新w e，每次就用这么一个简单的这么一个方程式，你就能更新。

如果我们知道这个p的这个这个导数啊，p对w的导数，这个是p对这个z跟d的导数，如果我们知道这个p对w一的一个导数，我们就能求，就根据这个简单的更新的一个法则，这个阿尔法是我们的学习率，比如说0。03。

比如现在我的w1 ，我看他的这个纵坐标，比如说这是1。2，那就是1。2要更新到几呢，1。2要更新到1。2，减去一个阿尔法贝的啊，减去高alpha倍的，0。03倍的，这个p这个东西对w一的一个偏导数。

它是一个数啊，当你把w一这个1。2带进去的时候，它是一个数，就是屁对w一它它是一个函数，但是当我w一等于1。2的时候，它就是一个把函数带进去，它就是一个真实的实数了，我根据这个更新法则。

我就能把那么迭代的一步一步来，我就能把w就能他们就哒哒哒哒哒哒哒哒，就一直到了最优点了，这个是啊，这个是很通用的，任何的机器学科吧都是这么干的，那么神经网络难在哪，难在哪呢。

就难在了我这个这个东西怎么求，怎么求，如果是线性回归模型，这个东西很容易就求助了对吧，按住印的这个课啊，安卓应的线性回归，他就他他他在第一周就告诉你，这玩意儿怎么求了对吧。

那我也他妈的一神经网络的这个东西，我怎么求，人们整整花了20年才发现怎么求这个东西啊，也就是真正的张飞king呢，他们发明了这个这个这个bp算法之后，他们才知道啊，那能这么久，因为在以前的话。

当这个w特别多的时候，当这w参数，一个深度学习网络动辄几千万个这个w，你如果没有一个很有效的算法，你不会求这个p对w的偏导的话，你是没有办法找到一个最优的，这个w的这个值了。

那么人们整整差不多花了20多年才，突然有一天啊，其实很简单，突然有一天才发现哦，我有我这么做这么做这么做啊，他就可以了，而而这个算法呢是神经是呃，想想真正掌握神经网络的一个本质的，本质的本质。

所以我给大家，而很多的这个神经网络教程呢，讲到这儿就不讲了，就说啊用一种算法啊，你就能找到一个最优的，你接着往下做，因为他们想讲这个卷积网络的一些特征，但是啊嗯嗯很少有人把这个事情讲的比较明白。

那咱们今天就看看，其实它不难啊，那咱们看看怎么来玩这个事情啊，那么首先呢我们要干的第一件事啊，是这样干的，第一件事是先把这个这个东西再做，再做一个改变，怎么改，请看，就是说我现在的这个这个决定。

就是比如说我这是一个神经元细胞，这个细胞我现在把所有东西我线性求职，我加了和之后，我用一个很简单的这么一个零一的这阶梯函数，来决定我要不要fire出去，就是要不要嗯产生这个神经冲动。

这个函数有一个不好的地方是嗯，大家知道我们要求p对w的一个一个偏导，那么它里头的每一个环节最好都能求导，这玩意儿不能求导啊，在零的地方，所以我就把它我给他做一个转变，我给他变成一个能求导的。

长得又像它的一个函数，大家想想看，长得又像他又能求导是哪个函数呢，就是我们传说中的sigmoid函数，sigmoid函数的选择就是就是打这儿来的，sigma的函数，就是嗯嗯这个一。

它的函数等于一加强e的负阿尔法次方啊，就是这个seek moid，西格玛的函数，那么它在阿尔法等于零的时候，这个是1/2，他是0。5，阿尔法特别大的时候，它无限趋近于一，阿尔法特别小的时候呢。

它无限趋近于零，它其实相当于这个硬，这个是一个硬阈值，hard stre就相当于一个软阈值啊，它就比较软了，就是它是一个比较连续的光滑的一个好函数，这个是sigma的当时的一个嗯一个想法。

那么sigma能求导这件事，嗯是是我们能干后面的啊，这个事的一个一个关键，有问题没有，大家，怎么就求导，就是说我现在这个之前，这个阈值函数不是不能求导吗，我现在想想，想要对每一个东西能求导。

我就先把这个这个阈值这个函数，我给它变变成一个光滑的函数，光滑的函数还得长得有点像，他是谁呢，sigmoid，这就是sigmoid的选择它的一个原因，而且sigma的这个函数求导它他特别棒。

如果是如果y是1÷1加上e的负x的话，y撇等于什么，y撇等于y乘以1-1减y，就是说他的求导还还还非常酷，他的求导是相当于自己啊，阿尔法的系数刚好等于-1，不是这个函数的定义。

就是它sigmoid函数的定义，就是它这是一个函数嘛，我要用一个函数，就是如果这个图像长这个样子，它的解析式就是它好，为什么要在这个地方求导，马上会说马上就会说，那么现在我们就要以嗯。

人类历史上最简单的一个神经网络，我们来求一下这个东西，你就知道了，他是什么呢，请看好啊，他只有两个神经元数据，因为我们现在不考虑，就我们以前的x不不是一个向量吗，比如说100为乘以一的。

对应一个y值是1x1的，我们先不考虑这种复杂的矩阵运算，这个是真正把bp算法能理解的一个关键，你先别想复杂的，听我给你讲，你先讲简单的啊，x是一个实数啊，是个一一维的一个东西，y也是一个实数。

它也是一个一维的东西，我有一个x11 个f映射，是从实数到实数的一个映射，比如说是一个这样的一个函数，我们要干的是这件事情好吧，那我们要干这件事情，我们就可以建立一个最简单的神经网络。

这个神经网络的每一层只有一个神经元，听好了，请看，那么我们现在就就是说好在一个新板子上写，我们现在有一个有一个x对吧，有a x最简单的一个神经元呢，我x我要乘一个乘一个系数，对不对，乘一个w1 。

这w我这个结果呢我给它起个名字，那咱们就把它叫做p1 ，这个p一呢，我们要要经过一个这个，我刚才说这个sigmoid的这个这个，这个sigmoid的这个函数，这个阈值，这个函数。

就是说这个p一跑到这个阈值的这个函数里头，这个函数是什么呢，这个函数是一加上e的-2 x次方，你给我一个p，我就给你一个p，经过0~1变化之后呃，经过西格玛变变变换之后，一个0~1的一个值，对不对。

那么我们出来的这个东西呢，我们把它可以把它叫做y对吧，我们把这个y呢我们在又进行了一个，啊一个一个w啊，这应该w2 吧，啊这个w2 呢我们算出来一个激活呃，呃一个就是w2 乘以y的一个这个叫p2 。

咱跟他一样嘛，就是想跟他写写的是一样的，然后呢再来一个再来一个这个sigmoid这个函数，最后呢我们有一个输出是我们的z值，你如果只其实是一个神经元这个东西，这东西是一个神经元。

这个东西呢是第二个神经元对吧，我就不画了，那么这个东西就是我们的啊，最简单的一个具有一个隐藏的一个神经元，有问题没有，关于我写我画的这个图，大家有问题没有，超级酷啊，接下来我要给你干的事情，超级酷。

不是等等，是是什么意思，为什么，这不是就是神经网络的体系结构，我现在就假设我用这两个东西，我来我现在要干的事情是什么呢，我想让找找一组合适的w一跟w2 ，使得我来一个x。

我输出的z跟我真实值的这个d越近越好啊，不不是，跟我真实训练集中的这个x对应的这个数，越近越好，你可以你可以经过1万层，但是我为了告诉大家怎么样的找这些w，经过两层，是你只要经过1万层。

我今天黑板写不下啊，因为一次的话它就不是就不是神经网络了，一次就是一次就是线性回归模型，哥哥，你看哈啊，不是一次就是logistic回归，logistic回归就是我x乘了个w，用了个sigmoid。

对不对，p一跟阿尔法就是p一跟阿尔法没有什么关系，就是说我这个这个函数的这个函数的定义，可以是这样，你这个输这个这个阿尔法是输入，当当p等于阿尔法的时候，就当p是输入的时候，阿尔法就会变成p。

如果打好啊，好都明白了吧，都明白，开始我开始给你变戏法了啊，人类想了25年想明白的一个事，还有问题没有，那么我们现在其实就是想知道这个w2 跟w1 ，对不对，然后我们要记住我们的损失函数是什么。

我们损失函数是p对不对，这个是就是说我们损失函数，我们是想让这个p最大化，这个p是什么呢，p是啊，这个d跟z的这个差值的平方，对不对，没有问题吧，我们想让它最大化，ok啊，好好好，那我们现在开始了哈。

准备好，那么我们现在如果想让他最大化，其实归根结底还是要干的一个事儿，就是要干这个事儿，要求出来他们的偏导，求出偏导之后，我就能用这个随机梯度下降的方法，这个算法来迭代的更新它了，我要更新它之前。

我必须得知道它的偏导是什么，一旦知道偏导是什么，我立马就带到这个嗯随机梯度下降，这个这这个更新法则里头，我就能更新了，好那么我们看，那么我们其实现在就想知道的是p对这个p啊。

他对西分别要知道p对w2 的p的w2 的偏导，跟p对w的偏导对吧，你只有两个不知道的，你x是知道的啊，x是你的训练，你的输入z z是你的这个输出，而这个w一跟w2 是你要学出来的东西，对不对。

那么我们看这个图哈，看这个图是最好记的，你这个z啊，你这个p你这个p要对w2 进行偏导，p dw中间还隔着个z呢，怎么办啊，链式法则就在这p对w2 的偏导，它能是p到谁的偏导，p到z的偏导，对不对。

再乘以z到w2 ，有问题没有，就相当于这是一个恒恒等变换，p到w w2 的偏导就等于p到z的偏导，乘以z到w2 的偏导，而z到w2 的偏导是不是还能再写一部，请看z到w的偏导。

我给他就能替换成z对p2 ，就是你看w2 到w2 到z，其实是经过了一个p2 的，对不对，那么那么我们z对w2 的偏导，那么就能写成z对p2 的偏导，乘以p2 除以w2 的一个偏导，有问题没有。

这是第一个，这是第一个神奇的地方，就是它等于第一项乘以第二项乘以第三项，有问题没有，ok好我们看看哈p对z的偏导能不能求，能求p对z的偏导就是什么呢，p对z的偏导就是二的d减z吗，你都能带进去。

p对z的偏导是能求的，z对p的偏导能不能求z对p的偏导，能求啊，就是sigmoid函数的偏导，sigmoid函数的偏导不就等于这个sigmoid，就是嗯，如果我们把他那么就是f撇乘以一减f嘛对吧。

这个也是能求的，给我一个数，我就能求p2 队w2 ，能不能求p2 队w2 ，那就是y值啊，那也能求啊，对不对，就递归，这就谢谢我，我先不聊这个词，那么p l队w2 是能求的p2 w2 就是y值。

就是第一个神经元的激活值，就是他对他的偏导，对不对，那么一样的，我看着这个图我已经这么写了，我看着这个图，我闭着眼睛都能写出来，p对w的前景，这走的有点远了，但是没关系，来咱们长征一次，看看咱们能不能。

就是如果你这个真正看明白的话，你下下一个呢，你就你就写就行了，咱看图说话就行，首先p对w的一一的求导，肯定是p对这个z的求导，对不对，乘以z对这个p2 的求导对吧，那么再乘以谁呢，那么就是到了p2 了。

p2 再对y嘛对吧，那么就是p2 对y的求导，那么再乘以一个呢，就是y的又对谁呢，y那么就是对p一的求导，然后呢p一对谁呢，p一对w1 ，手撸就撸出来了，不用查书，不用查任何东西，你手你就能撸出来。

给大家用两分钟时间仔细看着这个黑板，想一下啊，画看一下它对应的这个这个地方，其实p p对w一的求导就是p对z的求导，然后就是z对p2 ，然后p2 对y，然后y对啊，这个p1 ，然后p一对w一啊。

就这么一个简单的事情，给大家这个啊一分钟的时间消化一下，我目前写到的东西啊，奇迹还没有发现，这不是bp啊，这不是这这不是bp的全部啊，bp的全部还他的经，这是他的，这是他的第二个巧妙的地方，我跟你说。

任何一个改变世界的东西，基本上三个三个三个巧妙的地方加起来嗯，就你就能你就能变变成一个miracle在里头，梯度下降还没还没到bp呢，别着急啊，不要着急啊，d怎么在里头没有d啊，我哪写d了哦。

你说的这个d啊d啊这个真实值啊，真实值和预测值，你要在这一项里头要要要算，梯度下降的关系啊啊好，那我明白了，梯度下降的关系是我任何一个函数，我要更新它的权重，我都需要知道一个求偏导这个东西。

我才能做s d d就是随机梯度下降这个算法，我要做s e d，我必须得知道他对他的偏导，而神经网络的偏导呢，以前以前大家一直不知道该怎么有效的来算啊，就是以前大家光盯着这个看。

光盯着p对w2 看p对w的p对w的偏导，看怎么看也看没有，这怎么弄啊，这么复杂，这怎么办啊，还没完啊，我还没讲完，好9。38再给两分钟的时间啊，提问啊，因为这块是，这个神经网络能够牛逼。

他要没有这个东西，他就怎么牛，他也牛不起来，并不是卷积啊，那些东西那些东西不重要啊，那些东西是，变量只有w啊，你看谁谁变了x变了吗，x没变吧啊x是你的输入啊，你变不了w能变吗，w是你模型的参数。

就是你现在的模型，其实认为x到z应该满足这种变化，就是x输入一个x，我先给他乘个w，再让他非线性变换，再乘个w再非线性变换，这是你的模型假设，而你这个模型里头能变的只有w一跟w2 ，其他的不能变。

p对z的偏导，这不写着呢吗，我告诉你一个函数p它是d减z的平方，d还是一个常数，请你告诉我p对z的偏导等于什么，这个应该是，这个你应该会吧啊，不是这个它就等于dp d z，就等于这个是变量这个常量。

那就是2d减z，这个偏导的意义就是用来更新w，我w我他对w的偏导求出来了，我要干嘛，我要扔到这个s c d这个函数里头，这个是很多初学者蒙的地方啊，就是哎我他妈求了偏导，怎么老师就把书合上了啊。

是因为gt还在下一章才才给你讲的，p1 p2 是什么，p p2 是我定义的东西，我们把x乘以w一的这个值叫p一啊，我起了个变量名字给他，把这个place holder相当于，啊对这就不是啊。

也如果一个数学界的一个警察在这儿的话，那么他肯定会说你这个符号不大对的，其实无所谓啊，咬我呀，真实只减预测值的平方跟预测只减真是一样啊，肯定是一样的，y的意思，这y的意思是我定义的，我输入一个p1 。

我给他做了一个非线性变换输出，这个东西我给他叫个y，就是所有东西咱都给它变变变量化，你好能只要能给它起名，你就对他就，我感觉大家嗯基本上都掌握了，目前来看的话，那么我们把这个东西我们在。

我们把这个东西我们再给它倒过来写一下，这就来这bp，这个这个这个这个动态规划就出来了哈，我们这不是倒着写的吗，我们给他正过来行吧，正过来写的话呢，那其实就是p对这个w2 的这个偏导。

他就应该等于这个p2 对这个w2 的偏导乘以，这个z对p2 的偏导乘以这个，那么这个p队第一层的这个权重的，w e的这个偏导，你要给他把那个顺序换一下，那就是，往上往前看了，记不住了啊。

p w e y p对对，那么就是，我这个因为你要记住，到了这个女的，你就要爆炸了，stack overflow，我看看啊，好没问题对，那么我们先给他这么反着写一下，没有问题。

就是你就是把a乘b乘c写成b乘c乘a嘛，对吧，那么我们现在的问题就在于什么呢，啊这个里头的每一项也都能求啊，我我那我把图也画到这吧，图画到这儿，大家好，对比着看，不用反复的翻了。

嗯比如说我们把图画到这儿吧，我们经过了一个非线性变换，生成了一个y值，然后又给它经过一个非线性变换，生成了一个z值对吧，我们看这个图说话哈，那么，没有问题吧，没有问题好，那么我们我们看哈。

那么当我们比如说当这个玩意儿复杂的时候，我们怎么办，问题就来了，问题怎么来了呢，比如说嗯当我们这个x有x1 x2 x n的时候，比如说我们先看两个吧，先看x一和x2 的时候。

那么如果x一跟x2 也要来这个东西，就是说我的输入如果是呃二维的话呢，我的输出啊是z一跟z2 ，然后那么y x也要再用另一个参数，我还要跟它这边交互，那么x2 呢还要跟他交互，就相当于我的神经网络。

就是说我的输入这是x1 ，这是x2 ，然后我隐藏节点，比如说是，如果是三个的话，我这两个是一对犬种啊，这三个是一队选中，这三个是一队选中对吧，那么我们就是有各种的这种这些路，这这一个圆圆。

这一个圆圈代表着就代表了一个激活函数，这些这些这些箭头呢，就代表着一个全职的一个值，这个就是说相当于我刚才这个图的一个，另外一种表示方法啊，那么我们当我们的路有这么多的时候，我们要求这个链式好。

当我们录有，就当我们的从x2 到z一的这个路，可能性特别多的时候，当你的层数越来越多的时候，我们要求偏导求的这些路的这个可能的路径，是以指数复杂度往上走的，这个是一个大问题，这个是一个大问题。

就是说当我们的这个层数增加的时候，我们的possible pass pass，它是呈指数级别增加的，这个是很多讲这个啊bp算法它忽略的地方，他没有把bp，真正为什么人想了20年才想出来的。

这个东西的真正的就是bp算法，它真正牛的地方，他是在解决了这个问题，就是说如果你用我刚才给你教的这个办法，当你层数多的时候，你这个是写不过来的，但是bp如果你这样看的话，你把它转过来。

就是你如果用这个递归的这样看的话，你会发现不管我x2 到z一是由哪条路过来的，我z一和z2 其实看见的只是p2 ，跟这个p比如说r一撇，只看见的是这两个东西，就是不管只要我这两个不就前面的这些。

我不管你怎么变，只要这两个我知道了，我z一到z2 就相当于这两个最后一层的，这最后一层的激活值，把我们的这个这个最后的这个东西，这个门给守住了，所以你就看见你，其实你会看见你看这求了个p对z的偏导。

对不对，动规就从这来的，我这求了一个p对z的偏导，这也求了一个，这求了一个z的p2 的偏导，哎我这也求了一个，就相当于我这个东西我不需要算，那算那么多次了，有多少次呢。

有这个number of就可能的这个路径的这个次数，等一个就就我就能减少计算，就能够直接的以指数复杂度，我就能降到了一个线性，就相当于你看我这个z就是我倒数最后一层，跟这个z的这个偏导，我算一次。

我其他的所有路我都不用算了，这个其实是动态规划思想的一个核心，就是说啊我bp的这个back也是从这儿来的，我倒着从后面倒着往前算，我后面这个只要算一次，我其他的这些方程式我都给他带入这个数就行。

我就不用再反复算了啊，这样的话那就大幅度减少了我们的计算时间，也就是说我的计算开销呢它是它是欧啊，这个啊number of hidden layer的，这个就爽了，这个就爽大方了。

就是说我的我的复杂度是跟我的层数，是指呈线性积极的这个增长，你看对吧，我这个东西我算过一次了，我算其他的参数是，因为我这个这个w很有可能千万级别，就是说任何一个深度神经网络的，我这个w是几千万个参数。

那么我这每一次我都要这么算的话，那那那就把人算死了，那么当我们就就相当于有一个bottom up的，这么一个一个一个接近，就是说我把这个东西我以这个呃呃呃。

back back propagate这种形式，我一层一层的算，我算完之后，我能在每一个参数中能反复用，这样的话呢是啊bb算法的一个核心，大家听明白了没有，就是说我这个东西我每一层对它的一个偏导。

我能对所有的参数进行重复利用，这个是牛逼的一个地方，而很多人都忽略了这个地方，很多人光知道啊，求个偏导，求偏导，你求偏导，你要用的不聪明，你也你也就了啊，比如说求了偏导之后，你每一个你都这么都。

你每一个都这么应求，当你的参数量非常多的时候，你你这你这算法也慢，他快就快在了，用动规的思想加上偏加上练链式求导法则，算一个之后全部用啊，就这么几件事就把就把活干了。

而现在好玩的是很多教神经网络的一些博客，不强调这一点，你不强调这一点就相当于你在嗯，萧伯纳有一句话是别人夸我的时候，我都会拘促不安，因为没夸在点子上啊，这个bp算法呢他就会觉得操，你们觉得我牛。

但是你没你没夸在点子上嗯，好那么现在看这个r n，现在最后我拖十分钟的糖，我们看一下啊，rn是什么呢，因为我们之前学的，不管是卷积神经网络也好，还是这个多层感知器也好，它都是一层就这么猛。

你就往前走就行了啊，你给我一个x，我拿到一个y，不管中间有几层，你可以n层无所谓，最后有一个碗，这个是所谓的啊，前置神经网络，feed forward neural network，大家用的很多。

包括卷积神经网络，当我们他这个是一些卷积操作的时候，他就是卷积神经网络，就是比如说我的输入是一个平面的一个图像，那我们我们的这个卷积层，其实他就从平面就转生了这个，三维的一些tensor。

比如我这图像也可是三维的，就是tensor到tensor，就是他从一个一维的就变成一个三维的三维的，之后又变了很多的三维了，大家不用管，因为卷积你们也用不上啊，最后再给他解卷，解卷成一个一个一个一维的。

然后再对这一位的再来再来几项呢，我们做一个分类或者回归，这都是啊，这个是c n n啊，这个是mlp montage or perception啊，这个都是大大幅度的嗯，大家在用这个东西。

但是有一个东西长得跟这俩都不大一样啊，啊啊啊，对你也可以铺里，对它是有铺领的，铺垫之后就是一个大块块变成一个小块块，我就不画了啊，无所谓啊，嗯好那马文是这样rn是这样，这是我唯一从网上拿的一个图哈。

怎么说好这样而是这样，我每一个时刻我来一个x，比如说我在t时刻我有个xt，我在t时刻呢有一个y t，然后我想干的事情是我每来一个t时刻，来来一个xt，我输出一个y t，但是我同时我又来了。

比如说x t加一，我这个网络呢能够记住我上一个时刻，xt的一些蛛丝马迹，当我看到xt加一的时候，我能够预测出来yy的这个t加一，能够啊体现出来这个模型，能够记住我上一时刻的一些东西。

哎这个这个事情相信很合理，对不对，就是他的这个intuition，他的这个背后的这个动机其实很合理，就是说当我在预测，我这个人是很喜欢举具体的例子，我不喜欢写写这种奇奇怪怪的下标。

咱们还是把它简单的写了，比如说x啊，在这个在在零，是唯一，但是我要求我要求这个模型呢，请你在预测唯一的时候，你能不能记住x0 的一些信息，来指导我预测x1 ，同理我到了x2 的时候。

你要预测y2 的时候，你这个模型能不能记住x0 和x一的信息，这两个信息同时来指导我预测y2 ，也就是说我在二二十克我预测y2 的时候，请你把之前的东西啊也都以某种形式，相当于以某种记忆啊，能够记住啊。

这个是rn的一个啊，关键关于这个r n的关键，大家有问题没有，就是他的这个关键假设，不好意思啊，刚才是在倒水，不是在尿尿，r n r n的全称是recurrent neural network。

叫做递归神经网络，怎么记住我，这就是我马上要要教大家的地方，瑞克伦neal network，他记住是这样，他想记住是这样，他想记住是这样，你看哈，因为你们要去看这个r n的话，r n的这个介绍不好懂。

比如说我这个要记住呢，就是说我需要给他一个参数，是你要往前面记多少，你要记十步，还是要记100步，还是要记1万步，这个是你的一个参数，如果你的参数给定了，比如说我每一次往前进四步啊，它长的就是这个样子。

这个网络呀，网络完全因为你们看到其实任何一个神经网络，它可以长成一个，只要你没有还他他都可以对吧，我这输入是啥啊啊啊x它是比如说是300维的，然后x我有一些权重啊，这个权重我把它叫做u对吧。

权重矩阵嘛就是我之前的这个东西就是什么，这个东西就是有问x先变成一个，这是比如说100维的，我要乘以一个100x300为的，那么就会变成一个300维的一个隐藏100维的，乘一个100维三维的矩阵。

然后再做一个非线性变换，这里也是一样的，我乘了一个u，这个u大概是300乘以hidden layer的这个size，就这么一个矩阵，对不对，300乘以黑灯亮，这个矩阵呢我给它映射到了隐含层。

这个隐含层要干的事情是什么呢，隐含层干的事情是我在这个时刻，我要预测出它所对应的y值来，那么怎么做的呢，那么就很简单，我这个隐含层的激活值，比如说我这是一，这是h比如说如果是嗯50的话。

那么就是50个隐隐含层，50个隐含层，我在给他project do一个神经元上，我就给他预测出去了对吧，这是那么我要干的事情是，这个神经网络其实是一个很复杂的一个，嗯这个样子的一个东西。

就是我长的是长的是这个逼一样，我到了下一个时刻的时候呢，下一个时刻的时候，我这又有一个矩阵叫w，就w把隐含层的这个这个输出，输入层的这个优质我同时的喂给这个隐含层。

也就是说这个隐含层这个隐含层这个hidden，这个hidden它其实是一个w乘以last，这个hidden t减一加上这个u乘以，你此时此刻x是这个东西，就是说我这个w跟隐含层的激活值。

我求了一个求了一个映射，它是100枚的对吧，y x跟u也求了一个这个映射，也是100维的，这俩100维的向量简单粗暴的加1+1起，加一起之后呢，我才把这个才把这个h输出出去，然后我要预测完的话。

我再给他接一个也行，你干嘛就行，然后这个w到他们也一样啊，然后完了那个关键假设，就是每一次这个隐含层之间的这个这个映射，跟x到隐含层的这个映射，不管时间它们都是同一个这个矩阵，w是学出来的一样。

你就通过bp就能学出来所有的参数，凡是你不知道的东西全都会通过训练集学出来，因为我看刚才已经会bp了，其实会bp的话，这个东西同样能够用链式法则能求导，能求出来，因为他是一个它是一个无环的一个拓扑结构。

都可以通过bp的算法能够求出来，那么如果用一种比较紧致的算法来，紧致的这个图模型来表示的话，它就是这个样子，有一个输入，这个输入呢这个神经元在这个节点，他根据这个输入啊啊，以及上一个时刻的。

这个上一个时刻的引节点的这个激活值，我求一个啊，求求一个w的这个变换，然后跟这个x值我加一块，我把它弄出个输出出来，这就是r n，而这个隐含节点可以是什么呢，可以是个sigmoid函数，这就是最简单的。

而，也可以是一个双曲函数，没有问题，但是现在大家最火的是一个叫什么，叫一个l s t m的一个东西，这个东西它的体系结构跟这玩意儿一模一样，它只有一个地方不一样，就是说以前我这个东西啊。

它是一个简单的一个函数嘛，就是它的激活值就是一个简单的sigmoid函数，对不对，l o s t m是说当我们这个层数特别多的时候，比如说当我们要往前看十个时间点的时候，你要求这个梯度的话。

你的数学能证明出来，它有一个叫做梯度消失问题，就求着留着他妈成零了，vanishing gradient problem，那么l s t m呢，就是他把这个每一个sigmoid这个函数给它，变成了一个。

他从一个sigmoid这个函数给它，变成了一个极其复杂的里头，又有一些其他参数的一些带带带带，一些各种的记忆门啊，什么门，那些一些小的一些函数，就相当于你作为成员来说，你可以不管。

就是你你你要只想用的话，你只需要知道这个sigmoid函数，变成了一个更加复杂的一些函数的集合，但是它它的输入输输出都是一样的，在这个curse里头，你只需要调用i s t m这个函数。

你你只需要告诉他，我输入了维度，你只需要告诉他，我往前看多少维和输出是什么东西就行，然后里头的你都不用管，尤其是这不是一个机器学习的课程，你就更不用管了啊，那么就是你要知道的是l s t m。

你如果要去维基百科去看l s t m的图啊，我强烈鼓励大家去看一下，一定要去看一下，看完之后你就你就算鬼鬼也看不懂的玩意儿，但其实它的本质呢就是这个东西。

啊l o s t m它叫做long shot memory，呃就是说是什么呢，就是说它的这个以前我们的神经网络的，每一个节点，它不是一个简单的一个非线性变换吧，他现在要干的事情是我们这个书啊。

这个hidden layer的这些书，我用一些用用四个函数，他一共用了四个函数，就是就在这个里头他又用了一些嗯，就是用了四个函数，这个这个是一个函数，一个sigmoid函数。

它这个东西是他用四个函数的一个整体，但是它的输入和输出跟sigmid是一样的，但是那么就是它里头有更多的一些，更多的一些奇奇怪怪的一些权重，是需要去学的啊，这个你不用管u跟v就是我们这个嗯好好。

u跟v就是我刚才说的这个我这个里头的，就是我这个里头的u跟w就他写的是这个东，u跟v就是这这个从x到引节点，我们不是得乘个矩阵嘛，就相当于我们把这些全值一个一个的全值。

把它每一个数我给它写成一个矩阵了啊，不不不不，递归神经网络和循环神经网络，是两个完全不一样的东西啊，一个叫做recursive，一个叫做瑞瑞科，neural network，一个叫做record。

嗯这个是做语法术的时候用的，这个你不管这个是纯粹n l p的事啊，这个事情是我们要干的，这个东西是跟时间有关的，建模这个这个叫做循环地呃，这个叫做这个叫做循环递归网络，这个是循环，不是循环。

然后这个叫做你的那个叫什么递归啊，这个叫做递归啊，这两个不大一样的事情，v是这样，你看哈v的话，我们这隐含层不是如果是100倍的，我这输入如果要是1v的，我怎么办，我100v再v我在就给他一再来一个呗。

就相当于这是一个100维的一个隐含节点，我输出是一个整跌，我怎么办，那我就给他100，给他给他活活的摁到这个一维中去，我再来一个激活值就出去了，怎么体现，记住我多个，你看哈，那么我们做bp的时候。

我们就会发现，如果我们要做，如果是这个体系结构，我们要做bp的时候，改变了这个这个时刻y值的东西，你要做bp这个链式求导求回去，你就会发现他也跟这个就是y t加三，他跟这比如w1 。

它跟它这个里头就有这些h一的这些项，那么你通过这个链式法则的这个这个公式，你就能看出来啊，当他变就当第一个时刻的黑人类要变的时候，第三时刻的y值会相应的变化，这就说明了它的这个时序的这个这个相关性。

因为你想毕竟一个一个一个东西，一个y对一个w他的这个偏导意味，他这个东西对w，比如说y y队w一的偏导的意义是什么，e是当我们w一变化，比如说这个数算下来，比如说是0。3，它的意义是当我w变换0。

3的时候，我呃w y变换回一的时候，我w变要变换是0。3，就它是一个这个梯度的这么一个一个作用，所以说当我们在t3 时刻y值变化的时候，我们能通过公式啊看到啊，是跟这个11时刻的这个变化是有关的。

所以我们就知道这个模型吧，第一时刻的这个嗯变化，它考虑进去了好最后在承诺大家跟大家说一下，这个跟大家说一下这个这个这个这个叫什么，演化计算能干的一个事情，嗯你知道能干的事情。

你看就是说我们进行了一些这个，这个是嗯八八十年代，他们就他们就搞出来的一个事情，就是说比如说我们有一个这个物理模拟的一个，一个环境啊，我们有一个物理模拟的环境，就是说这个呃物理定律都是写死的。

一个相当于一个仿真器，然后呢，我们用这个神经网络呢，来编码这些物体的运动的方向，来让他们学会在这些空间中进行运动，你看他们现在经过一些迭代，他们哎就而这些物体的长相呢，也是用神经网络长出来的。

就是他刚开始长的大概是比如说两个木块，然后又变成三个木块，然后又变成两个木块，就要求他们通过一些神经呃，呃通过一些这个遗传算法的迭代，长出来一种最适应海底这个环境的，这些物体的这个样子。

你看哎这这就是一些迭代，他长得帅的样子，而且他们哎这么动，这在什么时刻怎么动，也是用这个啊，genetic av是怎么说啊，变变出来的东西，你看哎经过差不多，这是四五百次迭代。

他们就完全就是长出来的一个新的生物，就是这个这个东西的掌法是啊，由由这个啊遗传算法所进化的，你看它在物理物理的这个环境。



![](img/5fffcd7617edf55f8c65cda7ac10ad9f_2.png)

他也知道啊，如果我的定义，fitness函数是在空间中进行走动的话，那么我就应该长成这个样子，并且这么走，就是说在这里的fitness的函数，就是根据物理定律来定义好的。



![](img/5fffcd7617edf55f8c65cda7ac10ad9f_4.png)

我要往前走这个事情啊，这个就是纯粹找个物理系的人，把牛三律全写进去啊，就行啊，你看这就是他们慢慢长出来的一些东西，而而且他的这个东西呢，他这个长相也是由遗传算法进化出来的。

比如说他这个东西的爸爸大概只有这个，只有两个两个木块啊，那然后他长着长长着长，他就知道我操长着长着呢，他他就知道这个这个东西需要这么多了啊啊，然后呢后面还有一个好玩的。

就是他们如果fitness函数是要抓这个小红点，就说你们能看见吗，你们你们现在能看不能能不能看见啊，no no no no no，好好好，那么如果他的fitness呢是呃捕捉这个小红点。

那么他们也就会再看这，当小红鸟动的时候，他们也学会哎这么动，我就能抓住这些小红点，非常酷啊，机器啊，你看这些奇奇怪怪的一些生物，他们就知道去追食捕食这些小红鸟啊，然后比较有意思的是。

这个就是说呃两个两个生物，你们要学习抢抢夺食物啊，学习出一种动作规范来来抢得这个食物一，并且学习出一种长相来谁，你看诶这个就比较傻的，他就一抱诶，然后这个还学雷锋，还还给对方推啊，这个比较聪明啊。

他他与其学习抢食物，他学习出来的规则是把对方推开，你们你们再看就把对方就一次就推开了，哎就把东西就糊住了啊，然后再看这个啊，这个他就哎哎就把对方就摁住了，这个就这个就一报啊。

反正都是很有意思的一些这个这个这个结果啊，你看哎，这是两个啊，这就拿走了，就这这一看就是比较贪心的一种啊，还有如果两个都比较攻击性的。



![](img/5fffcd7617edf55f8c65cda7ac10ad9f_6.png)

他们两个就就死锁了啊，就就就就死锁了，这个里头唯一写写进去写死就是物理定律啊，就是一个一个游戏的，任何一个游戏引擎都能干啊，这个这这个有意思，这个他与其抢食物，他学习出来的是把对方胖揍一顿啊。

诶这就拿拿到了啊，这这就比较悲哀了啊，拿不到就跑了啊，稍稍等回答这个问题啊，咱们先把这个看完，我永远看这个看不累啊，视频链接啊，可以给你们就搜这个car smith，优酷上只有一个人传了一个。

关于cos smith的这么一个视频好，那么我们现在回来回过头来看这个，今天的这个这个这个总结，我是否达到了这个目的，同时我看一下这个同学的问题，分段函数当然可以就是说求解出一个分段函数。



![](img/5fffcd7617edf55f8c65cda7ac10ad9f_8.png)

是这样，如果你要做分段函数的话，你这个是，你就这个是非常也是非常常见的一个算法，就比如说我们叫做peace wise，regression，就是我们不要再给它分出一些区间出来。

这个区间里头我们拟合一个线性函数，这个区间我们拟合一个线性函数，这个区间里头我们拟合一个线性函数，然后具具体做预测的时候来一个x，先看在哪个区间里头，然后再在这个区间里头对应的那个函数值，我们做一个。

你做做一个预测，然后汇报出来它的这个值没有问题，好那么我们今天的这个作业再回到作业哈，请用这个feature selection的这些方法，理论算法也好。

这个这个forward a selection也好，做一些啊，遗传算法啊，不是做一些特征选择的方式，同时注意看这个sk learn里头的这个，他给你写好的这个特征选择的函数，很多人光会用来唉。

很多人忽略了sk 2里头有已经写好的，非常棒的特征选择的这些函数，你要用这个也没有问题啊，同时呢再加上你自己的一些模型啊，嗯学有余力的，你可以就直接用这cross里的l s t m了啊。

但是你要你要会用啊，你用的不好的话，一般还就就很容易overfit，然后呢同时你汇报出来你的最优的结果，这个以上算法我们在以后的一些选股的过程中，也也会接收到啊，就是说这个东西大家今天记住了。

记住它的最最最本质的一些东西，我们以后会会有一些实践，然后今天在课后，我大概得明天早上，好像出点问题啊，那个东西在我的一个在我在，在我那个这个job box里，就是在我那个一个云盘里头。

我那我尽快的给大家，就是一个非常好的一个遗传算法的，一个实现各种遗传算法的一个python的一些程程序，好今天拖堂拖得比较久，最后再给大家这么多图，要是留在p会留会留这些图都会留在pdf上。

最后再给大家留两分钟的时间提问吧，啊这个我不知道，这个我没有做过统计，但是l o s t m在某某些问题上，确实做的还不错啊，就是主要是你的主要的参数，其实是你的这个体系结构的选择。

以及你啊l o s t m中每一个隐藏的这个数目，其他的都其实没有什么可选的地方啊，对l o s t m用一个叫i m s prop的一个算法，它其实，其实你这些都，这些你都可以不关心。

因为别人都帮你写好了，但是这个东西呢它它是约等于，你可以把它看成约等于s cd这个东西就算是啊，你就算是用用了一个更好的一个这个算法，但是你到最后你还是要求这个y对w的偏导，所以说呢这个东西。

而n跟cn的区别，就是我的这个这个网络长得不一样，虽然cn的cn跟他是完全不一样的，cn是每一个神经元只看一个部分啊，每一个神经元只看一个部分，然后有很多的所谓的很多的神经元啊。

就会长成一个一个tensor的一个样子，这个你可以不用管隐藏的数目，不不不，隐藏的数目指的是嗯，就是啊啊你啊，你也可以就是他这个呃这个input dimension的，就是说你比如说你x100 。

你要给它映射到多少倍对吧，为什么定义为隐藏，就是隐藏是一个名字而已，就是说y是我观测到的，x是我是输我里头的东西的一些激活植物，就把它叫做隐藏啊，选择特征的时候就是嗯不是选择特征的时候。

你其实用这个线性回归的这个算出来，这个二峰值，就能当做一个，不要就已经可以的一个一个指标了啊，这个不好说啊，这个你需要比如说你要分段，你到底分几多少，分在哪，这个这个有各种各样的方法。

比如说你用这个ko啊，灯灯叫做density estimation，来进行估计，或者用一些其各种的方法都可以做啊，就是说你可以考虑它的这个这边的函数，密密度在这边比较高啊，或者怎么样的。

就有有各种各样的方法来做这个事情，但是其实啊这些分分段啊，什么这些方法都是，其实是在嗯大数据之前的一些时代，就那么几个数啊，当你数据量很大的时候，你用任何一个非线性的一个模型，就能做的不错了嗯。

所以你讲都可以无所谓，这个有各种各样的奇淫技巧，你去看这个introduction to statistical learning，就是，这本书里头好像有专门一节，专门讲这个怎么样做。

怎么样用各种各样的方法来估计这个，分段的这个区间，这个会这个我刚才说过了，他是在我的一个云盘里头，我今天晚上不一定能翻明白啊，今天我还得再找一个啊对啊，他其实啊我其实想说的是这样。

我其实想说的是这个pc他干的事情，是他他把这个每一个数据投影在了，让它这个方差类类内方差最大的几个方向里头，但是我对就是这个主演量化同学，你是想把这个时间分为啊，比如说10年到15年是一个一个区间。

我给它拟合一下，15年到18年是一个区间，我给它拟合一下这个这个想法，可以，这个想法其实就是说我对不同的时间段，拟合了一些不同的函数，量化交易有一个就是最最唉这个这个东西了。

其实好的程序员跟好的量化交易员，他们的共性都是都是野生的，很少有谁说嗯通过几本书拿来说嗯，就是张无忌掉到了山洞里头，有个老猴子从肚子里颠出一本书来说，你练练一练，他就把九阳神功练成了，在编程。

还有机器学习以及做量化都没有，都没有这等好事，但是有一个叫这个，他写的几本书可以去看一看，但是他主要是以matlab的写的这个东西行，那今天就就到这里，今天是因为讲的是我比较喜欢的一个事情。

所以给大家拖了会儿糖啊。

![](img/5fffcd7617edf55f8c65cda7ac10ad9f_10.png)

就到就到这里。

![](img/5fffcd7617edf55f8c65cda7ac10ad9f_12.png)